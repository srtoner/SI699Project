{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephentoner/miniconda3/envs/si699proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gutenbergpy.gutenbergcache import GutenbergCache, GutenbergCacheTypes\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Helper as H\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "work_list = H.load_file('bookid_year.csv', 'csv', config['REPODIR'] + '/' + config['DATADIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list.head()\n",
    "guten_works = tuple(work_list['Gutenberg ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(config['REPODIR'])\n",
    "cache  = GutenbergCache.get_cache()\n",
    "# Sample Book Ids for Each author\n",
    "guten_data = [s for s in cache.native_query(\n",
    "    \"SELECT a.authorid, a.name, count(b.id) as book_count from \\\n",
    "    (SELECT * from authors \\\n",
    "    LEFT JOIN book_authors \\\n",
    "    ON id = authorid) as a \\\n",
    "    LEFT JOIN \\\n",
    "    (SELECT * FROM books \\\n",
    "    WHERE gutenbergbookid IN {}) as b \\\n",
    "    ON b.id = a.bookid;\".format(guten_works))]\n",
    "\n",
    "len(guten_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "guten_sample = [s for s in cache.native_query(\n",
    "    \"SELECT DISTINCT b.id, t.id, t.name, t.title, b.gutenbergbookid FROM (SELECT DISTINCT id, gutenbergbookid FROM books  \\\n",
    "    WHERE gutenbergbookid in {} ) as b \\\n",
    "    LEFT JOIN ( \\\n",
    "        SELECT a.name, a.id, a.bookid, titles.name as title FROM \\\n",
    "            (SELECT * from authors \\\n",
    "             LEFT JOIN book_authors \\\n",
    "             ON id = authorid) as a \\\n",
    "        LEFT JOIN titles \\\n",
    "        ON a.bookid = titles.bookid) AS t \\\n",
    "    ON t.bookid = b.id;\".format(guten_works))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(guten_sample).drop_duplicates(subset=[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/vw0sn5w90nnccbfn7bskv0jm0000gn/T/ipykernel_30665/4062553802.py:1: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  data_dict = df.rename(columns = {0:'bid', 1:'aid', 2:'author', 3:'title', 4:'gbid'}).merge(work_list, left_on = 'gbid', right_on='Gutenberg ID').to_dict(orient='record')\n"
     ]
    }
   ],
   "source": [
    "data_dict = df.rename(columns = {0:'bid', 1:'aid', 2:'author', 3:'title', 4:'gbid'}).merge(work_list, left_on = 'gbid', right_on='Gutenberg ID').to_dict(orient='record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR fetching Lady Bridget in the Never-Never Land: a story of Australian life\n",
      "ERROR fetching North and South\n"
     ]
    }
   ],
   "source": [
    "from SampleText import fetch_book, trim_book, partition_and_sample\n",
    "\n",
    "CHUNKS_PER_WORK = 60\n",
    "CHUNK_LENGTH = 50\n",
    "\n",
    "# TODO: keep rest of books for training word embeddings?\n",
    "data = []\n",
    "\n",
    "for work in data_dict:\n",
    "    try:\n",
    "        author_id = work['aid']\n",
    "        author_name = work['author']\n",
    "        book_id = work['bid']\n",
    "        gb_id = work['gbid']\n",
    "        title = work['title']\n",
    "        date = work['Date']\n",
    "        gender = work['Gender']\n",
    "        penname = work['Penname']\n",
    "        clean_book, _ = fetch_book(book_id)\n",
    "        original_len, trimmed_book = trim_book(clean_book)\n",
    "        excerpts, excerpt_lines = partition_and_sample(\n",
    "                                            trimmed_book, \n",
    "                                            CHUNKS_PER_WORK, \n",
    "                                            CHUNK_LENGTH, \n",
    "                                            original_len\n",
    "                                            )\n",
    "        for idx, passage in enumerate(excerpts):\n",
    "            data.append({\n",
    "                \"author_id\":author_id,\n",
    "                \"author_name\":author_name,\n",
    "                \"book_id\":book_id,\n",
    "                \"gutenbergbookid\":gb_id,\n",
    "                \"title\":title,\n",
    "                \"text\":passage,\n",
    "                \"text_lines\":excerpt_lines[idx],\n",
    "                \"decade\":str(date)[:3],\n",
    "                \"gender\":gender,\n",
    "                'penname':penname\n",
    "            })\n",
    "    except:\n",
    "        print(\"ERROR fetching {}\".format(title))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_v4.pkl\", \"wb\") as outfile:\n",
    "        pkl.dump(data, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch works for every book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = H.load_file('corpus.pkl', 'pkl', config['REPODIR'] + '//' + config['DATADIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = H.load_file('data.pkl', 'pkl', config['REPODIR'] + '//' + config['DATADIR'])\n",
    "data_text = [dat['text'] for dat in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=data_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si699proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e2ea11af011fc4c4235b2e3407140c249cc835cc2acc65071fa184af431f938"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
