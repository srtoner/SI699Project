{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928e8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephentoner/miniconda3/envs/si699proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gutenbergpy.gutenbergcache import GutenbergCache, GutenbergCacheTypes\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a643534",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentence_embed.pkl', 'rb') as f:\n",
    "    embed = pkl.load(f)\n",
    "\n",
    "embed_df = pd.DataFrame(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc33b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = embed_df.rename(columns = {0: 'seqid', 1: 'passage_key', 2: 'sent_embeddings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58d1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = U.load_file('data_vFFF.pkl', 'pkl', config['DATADIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbc9733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_id</th>\n",
       "      <th>gutenbergbookid</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lines</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>Sub_A</th>\n",
       "      <th>Sub_B</th>\n",
       "      <th>Sub_C</th>\n",
       "      <th>str_text_lines</th>\n",
       "      <th>passage_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "      <td>Austen, Jane</td>\n",
       "      <td>PG105</td>\n",
       "      <td>PG105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[Croft’s next words explained it to be Mr Went...</td>\n",
       "      <td>2371</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>{'Regency fiction', 'Ship captains -- Fiction'...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Love stories</td>\n",
       "      <td>Young women</td>\n",
       "      <td>Psychological fiction</td>\n",
       "      <td>2371</td>\n",
       "      <td>PG105_2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5049</td>\n",
       "      <td>Austen, Jane</td>\n",
       "      <td>PG105</td>\n",
       "      <td>PG105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[name and livery included; but I will not pret...</td>\n",
       "      <td>7471</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>{'Regency fiction', 'Ship captains -- Fiction'...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Love stories</td>\n",
       "      <td>Young women</td>\n",
       "      <td>Psychological fiction</td>\n",
       "      <td>7471</td>\n",
       "      <td>PG105_7471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5049</td>\n",
       "      <td>Austen, Jane</td>\n",
       "      <td>PG105</td>\n",
       "      <td>PG105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[and he seemed quite delighted, and, for my pa...</td>\n",
       "      <td>5021</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>{'Regency fiction', 'Ship captains -- Fiction'...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Love stories</td>\n",
       "      <td>Young women</td>\n",
       "      <td>Psychological fiction</td>\n",
       "      <td>5021</td>\n",
       "      <td>PG105_5021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5049</td>\n",
       "      <td>Austen, Jane</td>\n",
       "      <td>PG105</td>\n",
       "      <td>PG105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[, “Ah! Miss Anne, if it had pleased Heaven to...</td>\n",
       "      <td>2871</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>{'Regency fiction', 'Ship captains -- Fiction'...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Love stories</td>\n",
       "      <td>Young women</td>\n",
       "      <td>Psychological fiction</td>\n",
       "      <td>2871</td>\n",
       "      <td>PG105_2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5049</td>\n",
       "      <td>Austen, Jane</td>\n",
       "      <td>PG105</td>\n",
       "      <td>PG105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[fairly off, therefore, before she began to ta...</td>\n",
       "      <td>7921</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>2778.0</td>\n",
       "      <td>{'Regency fiction', 'Ship captains -- Fiction'...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Love stories</td>\n",
       "      <td>Young women</td>\n",
       "      <td>Psychological fiction</td>\n",
       "      <td>7921</td>\n",
       "      <td>PG105_7921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id   author_name book_id gutenbergbookid       title  \\\n",
       "0       5049  Austen, Jane   PG105           PG105  Persuasion   \n",
       "1       5049  Austen, Jane   PG105           PG105  Persuasion   \n",
       "2       5049  Austen, Jane   PG105           PG105  Persuasion   \n",
       "3       5049  Austen, Jane   PG105           PG105  Persuasion   \n",
       "4       5049  Austen, Jane   PG105           PG105  Persuasion   \n",
       "\n",
       "                                                text  text_lines  \\\n",
       "0  [Croft’s next words explained it to be Mr Went...        2371   \n",
       "1  [name and livery included; but I will not pret...        7471   \n",
       "2  [and he seemed quite delighted, and, for my pa...        5021   \n",
       "3  [, “Ah! Miss Anne, if it had pleased Heaven to...        2871   \n",
       "4  [fairly off, therefore, before she began to ta...        7921   \n",
       "\n",
       "   authoryearofbirth  authoryearofdeath  downloads  \\\n",
       "0             1775.0             1817.0     2778.0   \n",
       "1             1775.0             1817.0     2778.0   \n",
       "2             1775.0             1817.0     2778.0   \n",
       "3             1775.0             1817.0     2778.0   \n",
       "4             1775.0             1817.0     2778.0   \n",
       "\n",
       "                                            subjects    topic         Sub_A  \\\n",
       "0  {'Regency fiction', 'Ship captains -- Fiction'...  Fiction  Love stories   \n",
       "1  {'Regency fiction', 'Ship captains -- Fiction'...  Fiction  Love stories   \n",
       "2  {'Regency fiction', 'Ship captains -- Fiction'...  Fiction  Love stories   \n",
       "3  {'Regency fiction', 'Ship captains -- Fiction'...  Fiction  Love stories   \n",
       "4  {'Regency fiction', 'Ship captains -- Fiction'...  Fiction  Love stories   \n",
       "\n",
       "         Sub_B                  Sub_C str_text_lines passage_key  \n",
       "0  Young women  Psychological fiction           2371  PG105_2371  \n",
       "1  Young women  Psychological fiction           7471  PG105_7471  \n",
       "2  Young women  Psychological fiction           5021  PG105_5021  \n",
       "3  Young women  Psychological fiction           2871  PG105_2871  \n",
       "4  Young women  Psychological fiction           7921  PG105_7921  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a464c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = embed_df.merge(data_df, how= 'left', left_on= 'passage_key', right_on = 'passage_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652a0f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(embed_df.author_id.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1716fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# label_encoder=OneHotEncoder(sparse_output=False)\n",
    "label_encoder=OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792c7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= label_encoder.fit_transform(embed_df['author_id'].to_numpy(dtype='int32').reshape(-1,1))\n",
    "X = embed_df['sent_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dce9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61438e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state =699\n",
    "\n",
    "X_train, X_test, y_train, y_test = U.train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = U.train_test_split(X_train, y_train, test_size=val_size/(1-test_size),\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "149ee562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ff590db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed_df.sent_embeddings.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if (device == \"cuda:0\" or device == 'mps') else {}\n",
    "collate_func = lambda x: tuple(x_.to(device) for x_ in default_collate(x)) if device != \"cpu\" else default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c7951f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6330113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname, n_classes):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embeddings_fname = vocab_size        \n",
    "        \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "\n",
    "        # trained_weights = torch.load(embeddings_fname)['target_embeddings.weight']\n",
    "\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(trained_weights, freeze = False)\n",
    "        # self.embeddings = nn.Embedding()\n",
    "        self.linear = nn.Linear(num_heads * embedding_size, n_classes)\n",
    "\n",
    "        self.attention = torch.rand(self.num_heads, self.embedding_size, requires_grad = True, device=device)\n",
    "        \n",
    "    def forward(self, w):\n",
    "        w = w.squeeze()\n",
    "        # w = torch.t(self.embeddings(word_ids).squeeze()) # Embedding_Dim \n",
    "        r = torch.matmul(self.attention, w)\n",
    "        a = torch.softmax(r, 1)\n",
    "        reweighted = a @ w.T\n",
    "        output = self.linear(reweighted.view(-1))\n",
    "\n",
    "        return torch.softmax(output, dim=0), a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "302ac026",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "datasets['train'] = list(zip(X_train, y_train))\n",
    "datasets['val'] = list(zip(X_val, y_val))\n",
    "datasets['test'] = list(zip(X_test, y_test))\n",
    "\n",
    "train_list = datasets['train']\n",
    "val_list = datasets['val']\n",
    "test_list = datasets['test']\n",
    "\n",
    "model = DocumentAttentionClassifier(1, 50, 4, 'trained_model_final', n_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "be9eaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data, kwargs):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    Eval Data must be in DataLoader-ready format\n",
    "    '''\n",
    "\n",
    "    eval_loader = DataLoader(eval_data, batch_size = 1, shuffle = False, collate_fn=collate_func, **kwargs)\n",
    "\n",
    "    threshold = 0.2\n",
    "    probs  = np.zeros(len(eval_loader))\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, x in enumerate(eval_loader):\n",
    "            word_ids, label = x\n",
    "            labels.append(label.cpu().numpy())\n",
    "            output, weights = model(word_ids)\n",
    "            probs[idx] = output.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array([1 if p >= threshold else 0 for p in probs], dtype = int)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return labels, y_pred, f1_score(labels, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "80b83a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable function object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m loss_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[0;32m---> 32\u001b[0m     word_ids, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     33\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable function object"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_period = 500\n",
    "# model = model.to(device)\n",
    "writer = SummaryWriter()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# VVV GOLD STANDARD VVV\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 5e-3, weight_decay = 0.01)\n",
    "# ^^^ GOLD STANDARD ^^^\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr = 5e-3, weight_decay = 0.001)\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters())\n",
    "# optimizer = optim.RMSprop(model.parameters(), 5e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 5e-4)\n",
    "\n",
    "train_loader = DataLoader(train_list, batch_size=1, shuffle=True, collate_fn=collate_func, **kwargs)\n",
    "n_epochs = 10\n",
    "# n_epochs = 1\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}\n",
    "loss_idx = 0\n",
    "loss_record = []\n",
    "model.train()\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}r\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    for step, data in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        word_ids, labels = data\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        output, weights = model(word_ids)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to tensorboard. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "\n",
    "        if not step % loss_period and step:\n",
    "            writer.add_scalar(\"Loss\", loss_sum, loss_idx)\n",
    "            # if not step % (loss_period * 10) and step:\n",
    "            #     model.eval()\n",
    "            #     _y, _y2, f1 = run_eval(model, dev_list, kwargs)\n",
    "            #     writer.add_scalar(\"F1\", f1, loss_idx)\n",
    "            #     model.train()\n",
    "            loss_record.append(loss_sum)\n",
    "            loss_sum = 0\n",
    "            loss_idx += 1\n",
    "            \n",
    "\n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred, f1 = run_eval(model, val_list, kwargs)\n",
    "print(\"F1 Score of : \"+ str(f1))\n",
    "\n",
    "y_true, y_pred, f1 = run_eval(model, test_list, kwargs)\n",
    "print(\"F1 Score of : \"+ str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].squeeze().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si699proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
