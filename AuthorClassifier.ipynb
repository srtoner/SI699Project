{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "928e8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.gutenbergcache import GutenbergCache, GutenbergCacheTypes\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a643534",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_data.pkl', 'rb') as f:\n",
    "    embed = pkl.load(f)\n",
    "\n",
    "embed_df = pd.DataFrame(embed)\n",
    "\n",
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder=OneHotEncoder(sparse_output=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "792c7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= label_encoder.fit_transform(embed_df['author_id'].to_numpy(dtype='int32').reshape(-1,1))\n",
    "X = embed_df['sent_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "61438e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state =699\n",
    "\n",
    "X_train, X_test, y_train, y_test = U.train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = U.train_test_split(X_train, y_train, test_size=val_size/(1-test_size),\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "149ee562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ff590db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed_df.sent_embeddings.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6118d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if (device == \"cuda:0\" or device == 'mps') else {}\n",
    "collate_func = lambda x: tuple(x_.to(device) for x_ in default_collate(x)) if device != \"cpu\" else default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c7951f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6330113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, embeddings_fname, n_classes):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embeddings_fname = vocab_size        \n",
    "        \n",
    "        # Create the Embedding object that will hold our word embeddings that we\n",
    "        # learned in word2vec. This embedding object should have the same size\n",
    "        # as what we learned before. However, we don't to start from scratch! \n",
    "        # Once created, load the saved (word2vec-based) parameters into the object\n",
    "        # using load_state_dict.\n",
    "\n",
    "        # trained_weights = torch.load(embeddings_fname)['target_embeddings.weight']\n",
    "\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(trained_weights, freeze = False)\n",
    "        # self.embeddings = nn.Embedding()\n",
    "        self.linear = nn.Linear(num_heads * embedding_size, n_classes)\n",
    "\n",
    "        self.attention = torch.rand(self.num_heads, self.embedding_size, requires_grad = True, device=device)\n",
    "        \n",
    "    def forward(self, w):\n",
    "        w = w.squeeze()\n",
    "        # w = torch.t(self.embeddings(word_ids).squeeze()) # Embedding_Dim \n",
    "        r = torch.matmul(self.attention, w)\n",
    "        a = torch.softmax(r, 1)\n",
    "        reweighted = a @ w.T\n",
    "        output = self.linear(reweighted.view(-1))\n",
    "\n",
    "        return torch.softmax(output, dim=0), a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "302ac026",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "datasets['train'] = list(zip(X_train, y_train))\n",
    "datasets['val'] = list(zip(X_val, y_val))\n",
    "datasets['test'] = list(zip(X_test, y_test))\n",
    "\n",
    "train_list = datasets['train']\n",
    "val_list = datasets['val']\n",
    "\n",
    "model = DocumentAttentionClassifier(1, 50, 4, 'trained_model_final', n_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "be9eaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data, kwargs):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    Eval Data must be in DataLoader-ready format\n",
    "    '''\n",
    "\n",
    "    eval_loader = DataLoader(eval_data, batch_size = 1, shuffle = False, collate_fn=collate_func, **kwargs)\n",
    "\n",
    "    threshold = 0.2\n",
    "    probs  = np.zeros(len(eval_loader))\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, x in enumerate(eval_loader):\n",
    "            word_ids, label = x\n",
    "            labels.append(label.cpu().numpy())\n",
    "            output, weights = model(word_ids)\n",
    "            probs[idx] = output.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array([1 if p >= threshold else 0 for p in probs], dtype = int)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return labels, y_pred, f1_score(labels, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "80b83a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable function object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m loss_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[0;32m---> 32\u001b[0m     word_ids, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     33\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable function object"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_period = 500\n",
    "# model = model.to(device)\n",
    "writer = SummaryWriter()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# VVV GOLD STANDARD VVV\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 5e-3, weight_decay = 0.01)\n",
    "# ^^^ GOLD STANDARD ^^^\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr = 5e-3, weight_decay = 0.001)\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters())\n",
    "# optimizer = optim.RMSprop(model.parameters(), 5e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 5e-4)\n",
    "\n",
    "train_loader = DataLoader(train_list, batch_size=1, shuffle=True, collate_fn=collate_func, **kwargs)\n",
    "n_epochs = 10\n",
    "# n_epochs = 1\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}\n",
    "loss_idx = 0\n",
    "loss_record = []\n",
    "model.train()\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}r\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    for step, data in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        word_ids, labels = data\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        output, weights = model(word_ids)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to tensorboard. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "\n",
    "        if not step % loss_period and step:\n",
    "            writer.add_scalar(\"Loss\", loss_sum, loss_idx)\n",
    "            # if not step % (loss_period * 10) and step:\n",
    "            #     model.eval()\n",
    "            #     _y, _y2, f1 = run_eval(model, dev_list, kwargs)\n",
    "            #     writer.add_scalar(\"F1\", f1, loss_idx)\n",
    "            #     model.train()\n",
    "            loss_record.append(loss_sum)\n",
    "            loss_sum = 0\n",
    "            loss_idx += 1\n",
    "            \n",
    "\n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred, f1 = run_eval(model, val_list, kwargs)\n",
    "print(\"F1 Score of : \"+ str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].squeeze().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si699proj",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
