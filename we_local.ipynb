{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>['to header material.', '', '***', '', \"Lincol...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494</td>\n",
       "      <td>['I have no lawful right to do so, and I have ...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494</td>\n",
       "      <td>['for a minority of their own will secede from...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494</td>\n",
       "      <td>['in both cases, and a few break over in each....</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>['There is some difference of opinion whether ...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>2785</td>\n",
       "      <td>['   And, from this land of worship that I sin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>2785</td>\n",
       "      <td>['', ' And yet I know that far in that wood', ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>2785</td>\n",
       "      <td>['          Sometimes I feel that death is ver...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>2785</td>\n",
       "      <td>['', ' I think how many are alive', ' Who bett...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>2785</td>\n",
       "      <td>[' No. I will pass. Again', ' Of my delight in...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31997 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                               text  \\\n",
       "0            494  ['to header material.', '', '***', '', \"Lincol...   \n",
       "1            494  ['I have no lawful right to do so, and I have ...   \n",
       "2            494  ['for a minority of their own will secede from...   \n",
       "3            494  ['in both cases, and a few break over in each....   \n",
       "4            494  ['There is some difference of opinion whether ...   \n",
       "...          ...                                                ...   \n",
       "31992       2785  ['   And, from this land of worship that I sin...   \n",
       "31993       2785  ['', ' And yet I know that far in that wood', ...   \n",
       "31994       2785  ['          Sometimes I feel that death is ver...   \n",
       "31995       2785  ['', ' I think how many are alive', ' Who bett...   \n",
       "31996       2785  [' No. I will pass. Again', ' Of my delight in...   \n",
       "\n",
       "               topic  \n",
       "0      United States  \n",
       "1      United States  \n",
       "2      United States  \n",
       "3      United States  \n",
       "4      United States  \n",
       "...              ...  \n",
       "31992            NaN  \n",
       "31993            NaN  \n",
       "31994            NaN  \n",
       "31995            NaN  \n",
       "31996            NaN  \n",
       "\n",
       "[31997 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas dataframe\n",
    "\n",
    "\n",
    "df = pd.read_csv('baby_data.csv')\n",
    "# df = U.load_file('data_w_subj.csv', 'csv', config['DATADIR'])\n",
    "# df = pd.read_csv('/Users/jeffereyreng/Desktop/SI_699/final_project/data/data_w_subj.csv')\n",
    "\n",
    "# Select only the \"author_name\", \"text\", and \"subjects\" columns\n",
    "new_df = df[['author_id', 'text', 'topic']]\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "new_df.to_csv('text_subj.csv', index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>2319</td>\n",
       "      <td>['that I must come and see Frank a great deal....</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20779</th>\n",
       "      <td>4000</td>\n",
       "      <td>['was to find out who I was for the gentleman ...</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23973</th>\n",
       "      <td>463</td>\n",
       "      <td>['constantly obtained over an army of seven th...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14265</th>\n",
       "      <td>972</td>\n",
       "      <td>['Atlantic to France, was a long voyage for a ...</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21713</th>\n",
       "      <td>30</td>\n",
       "      <td>['\"I would prefer that he should give up the m...</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                               text    topic\n",
       "5841        2319  ['that I must come and see Frank a great deal....  Fiction\n",
       "20779       4000  ['was to find out who I was for the gentleman ...  Fiction\n",
       "23973        463  ['constantly obtained over an army of seven th...  History\n",
       "14265        972  ['Atlantic to France, was a long voyage for a ...  Fiction\n",
       "21713         30  ['\"I would prefer that he should give up the m...  Fiction"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('text_subj.csv', usecols=['author_id', 'text', 'topic'])\n",
    "\n",
    "# Shuffle the dataframe\n",
    "seed = 42  # set the random seed for reproducibility\n",
    "df = shuffle(df, random_state=seed)\n",
    "\n",
    "# Print the first few rows of the shuffled dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y_actual= label_encoder.fit_transform(df['author_id'])\n",
    "df['target']=y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW = CountVectorizer()\n",
    "BOW_transformation = BOW.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_ngram(n_gram,X_train=df['text']):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n",
    "    x_train_vec = vectorizer.fit_transform(X_train)\n",
    "    return x_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying tfidf with 1-gram, and 2-gram\n",
    "tfidf_1g_transformation= tfidf_ngram(1,X_train=df['text'])\n",
    "tfidf_2g_transformation= tfidf_ngram(2,X_train=df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephentoner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import packages\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['[', \"'that\", 'i', 'must', 'come', 'and', 'see', 'frank', 'a', 'great', 'deal', '.', \"'\", ',', '``', ',', \"'now\", ',', 'although', 'mr.', 'fitz-simons', ',', 'for', 'whom', 'i', 'was', 'mistaken', ',', 'is', 'as', 'low', \"'\", ',', \"'\", 'a', 'fellow', 'as', 'ever', 'came', 'out', 'of', 'dublin', ',', 'and', 'having', 'been', 'a', 'captain', 'in', \"'\", ',', '``', 'somebody', \"'s\", 'army', ',', 'is', 'now', 'a', 'blackleg', 'and', 'horse-dealer', 'by', 'profession', ';', 'yet', ',', \"''\", ',', \"'if\", 'i', 'had', 'brought', 'him', 'home', 'to', 'mrs.', 'fitz-boodle', 'to', 'dinner', ',', 'i', 'should', 'have', \"'\", ',', \"'liked\", 'far', 'better', 'that', 'that', 'imaginary', 'lady', 'should', 'have', 'received', 'him', 'with', \"'\", ',', '``', 'decent', 'civility', ',', 'and', 'not', 'insulted', 'the', 'stranger', 'within', 'her', 'husband', \"'s\", \"''\", ',', \"'gates\", '.', 'and', ',', 'although', 'it', 'was', 'delightful', 'to', 'be', 'received', 'so', 'cordially', \"'\", ',', '``', 'when', 'the', 'mistake', 'was', 'discovered', ',', 'yet', 'i', 'found', 'that', 'all', 'berry', \"'s\", 'old', \"''\", ',', \"'acquaintances\", 'were', 'by', 'no', 'means', 'so', 'warmly', 'welcomed', ';', 'for', 'another', 'old', \"'\", ',', \"'school-chum\", 'presently', 'made', 'his', 'appearance', ',', 'who', 'was', 'treated', 'in', 'a', 'very', \"'\", ',', \"'different\", 'manner', '.', \"'\", ',', '``', ',', \"'this\", 'was', 'no', 'other', 'than', 'poor', 'jack', 'butts', ',', 'who', 'is', 'a', 'sort', 'of', 'small', 'artist', \"'\", ',', \"'and\", 'picture-dealer', 'by', 'profession', ',', 'and', 'was', 'a', 'dayboy', 'at', 'slaughter', 'house', \"'\", ',', \"'when\", 'we', 'were', 'there', ',', 'and', 'very', 'serviceable', 'in', 'bringing', 'in', 'sausages', ',', \"'\", ',', \"'pots\", 'of', 'pickles', ',', 'and', 'other', 'articles', 'of', 'merchandise', ',', 'which', 'we', 'could', 'not', \"'\", ',', \"'otherwise\", 'procure', '.', 'the', 'poor', 'fellow', 'has', 'been', 'employed', ',', 'seemingly', ',', 'in', 'the', \"'\", ',', \"'same\", 'office', 'of', 'fetcher', 'and', 'carrier', 'ever', 'since', ';', 'and', 'occupied', 'that', 'post', \"'\", ',', \"'for\", 'mrs.', 'berry', '.', 'it', 'was', ',', '“', 'mr', '.', 'butts', ',', 'have', 'you', 'finished', 'that', 'drawing', 'for', \"'\", ',', '``', 'lady', 'pash', \"'s\", 'album', '?', '”', 'and', 'butts', 'produced', 'it', ';', 'and', ',', '“', 'did', 'you', 'match', 'the', 'silk', \"''\", ',', '``', 'for', 'me', 'at', 'delille', \"'s\", '?', '”', 'and', 'there', 'was', 'the', 'silk', ',', 'bought', ',', 'no', 'doubt', ',', 'with', 'the', \"''\", ',', '``', 'poor', 'fellow', \"'s\", 'last', 'five', 'francs', ';', 'and', ',', '“', 'did', 'you', 'go', 'to', 'the', 'furniture-man', 'in', \"''\", ',', \"'the\", 'rue', 'st.', 'jacques', ';', 'and', 'bring', 'the', 'canary-seed', ',', 'and', 'call', 'about', 'my', \"'\", ',', '``', 'shawl', 'at', 'that', 'odious', 'dawdling', 'madame', 'fichet', \"'s\", ';', 'and', 'have', 'you', 'brought', 'the', \"''\", ',', \"'guitar-strings\", '?', '”', \"'\", ',', '``', ',', '``', 'butts', 'had', \"n't\", 'brought', 'the', 'guitar-strings', ';', 'and', 'thereupon', 'mrs.', 'berry', \"'s\", \"''\", ',', \"'countenance\", 'assumed', 'the', 'same', 'terrible', 'expression', 'which', 'i', 'had', 'formerly', \"'\", ',', \"'remarked\", 'in', 'it', ',', 'and', 'which', 'made', 'me', 'tremble', 'for', 'berry', '.', \"'\", ',', '``', ',', '``', '“', 'my', 'dear', 'angelica', ',', '”', 'though', 'said', 'he', 'with', 'some', 'spirit', ',', '“', 'jack', 'butts', 'is', \"n't\", \"''\", ',', \"'\", 'a', 'baggage-waggon', ',', 'nor', 'a', 'jack-of-all-trades', ';', 'you', 'make', 'him', 'paint', 'pictures', \"'\", ',', '``', 'for', 'your', 'women', \"'s\", 'albums', ',', 'and', 'look', 'after', 'your', 'upholsterer', ',', 'and', 'your', \"''\", ',', \"'canary-bird\", ',', 'and', 'your', 'milliners', ',', 'and', 'turn', 'rusty', 'because', 'he', 'forgets', 'your', \"'\", ',', \"'last\", 'message.', '”', \"'\", ',', '``', ',', '``', '“', 'i', 'did', 'not', 'turn', 'rusty', ',', 'frank', ',', 'as', 'you', 'call', 'it', 'elegantly', '.', 'i', \"'m\", 'very', 'much', \"''\", ',', \"'obliged\", 'to', 'mr.', 'butts', 'for', 'performing', 'my', 'commissions', '--', 'very', 'much', 'obliged', '.', \"'\", ',', \"'and\", 'as', 'for', 'not', 'paying', 'for', 'the', 'pictures', 'to', 'which', 'you', 'so', 'kindly', 'allude', ',', \"'\", ',', \"'frank\", ',', '_i_', 'should', 'never', 'have', 'thought', 'of', 'offering', 'payment', 'for', 'so', 'paltry', 'a', \"'\", ',', '``', 'service', ';', 'but', 'i', \"'m\", 'sure', 'i', 'shall', 'be', 'happy', 'to', 'pay', 'if', 'mr.', 'butts', 'will', 'send', 'me', \"''\", ',', \"'in\", 'his', 'bill.', '”', \"'\", ',', '``', ',', \"'\", '“', 'by', 'jove', ',', 'angelica', ',', 'this', 'is', 'too', 'much', '!', '”', 'bounced', 'out', 'berry', ';', 'but', 'the', 'little', \"'\", ',', '``', 'matrimonial', 'squabble', 'was', 'abruptly', 'ended', ',', 'by', 'berry', \"'s\", 'french', 'man', 'flinging', \"''\", ',', \"'open\", 'the', 'door', 'and', 'announcing', 'miladi', 'pash', 'and', 'doctor', 'dobus', ',', 'which', 'two', \"'\", ',', \"'personages\", 'made', 'their', 'appearance', '.', \"'\", ',', '``', ']'], tags=[0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in df['text']:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "\n",
    "# Convert tokenized document into gensim formated tagged data\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size=50, window=2, min_count=1, workers=4, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.utils import shuffle\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# from nltk.tokenize import word_tokenize\n",
    "#\n",
    "# # Load the CSV file into a pandas dataframe and shuffle the rows\n",
    "# df = pd.read_csv('text_subj.csv', usecols=['book_id', 'text', 'subjects'])\n",
    "# df = shuffle(df, random_state=42)\n",
    "#\n",
    "# # Train a Doc2Vec model on the text data\n",
    "# documents = [TaggedDocument(word_tokenize(row.lower()), [i]) for i, row in enumerate(df['text'])]\n",
    "# model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4, epochs=10)\n",
    "#\n",
    "# def get_doc2vec_vector(df):\n",
    "#     doc2vec_vectors=[]\n",
    "#     for sentence in df['text']:\n",
    "#         doc2vec_vectors.append(model.infer_vector(word_tokenize(sentence.lower())))\n",
    "#     return doc2vec_vectors\n",
    "# doc2vec_vectors=get_doc2vec_vector(df['text'])\n",
    "# len(doc2vec_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31997\n"
     ]
    }
   ],
   "source": [
    "def get_doc2vec_vector(df):\n",
    "    doc2vec_vectors = []\n",
    "    for sentence in df['text']:\n",
    "        doc2vec_vectors.append(model.infer_vector(word_tokenize(sentence.lower())))\n",
    "    return doc2vec_vectors\n",
    "\n",
    "# Generate Doc2Vec vectors for the \"text\" column of the dataframe\n",
    "doc2vec_vectors = get_doc2vec_vector(df)\n",
    "\n",
    "# Print the number of generated vectors\n",
    "print(len(doc2vec_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU\n",
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'subjects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[CLS] \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m sentence \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m [SEP]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n\u001b[0;32m---> 18\u001b[0m labels \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49msubjects\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     20\u001b[0m \u001b[39m# Initialize the tokenizer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'subjects'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# from keras_preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df['text'].values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.subjects.values\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Tokenize the sentences and put them in the list tokenized_texts\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "hidden_states=[]\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "# Convert the ids into a tensor representation\n",
    "batch_size = 4\n",
    "input_tensor = torch.tensor(input_ids)\n",
    "masks_tensor = torch.tensor(attention_masks)\n",
    "train_data = TensorDataset(input_tensor, masks_tensor)\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "# Initialize the model\n",
    "if torch.cuda.is_available():\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True,).to('cuda')\n",
    "else:\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True,).to('cpu')\n",
    "model.eval()\n",
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, masks in dataloader:\n",
    "    torch.cuda.empty_cache() # empty the gpu memory\n",
    "    # Transfer the batch to gp\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.to('cuda')\n",
    "        masks = masks.to('cuda')\n",
    "    # Run inference on the batch\n",
    "    output = model(input, attention_mask=masks)\n",
    "    # Transfer the output to CPU again and convert to numpy\n",
    "    output = output[0].cpu().detach().numpy()\n",
    "    # Store the output in a list\n",
    "    outputs.append(output)\n",
    "# Concatenate all the lists within the list into one list\n",
    "outputs = [x for y in outputs for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2444, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vectors=np.array(outputs)\n",
    "bert_vectors=bert_vectors.mean(axis=1)\n",
    "bert_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "def get_vectors_pretrained(df, model):\n",
    "    embedding_vectors = []\n",
    "    for partition in df['text']:\n",
    "        sentence = []\n",
    "        for word in partition.split(' '):\n",
    "            try:\n",
    "                sentence.append(model[word])\n",
    "            except:\n",
    "                pass\n",
    "        sentence = np.array(sentence)\n",
    "        sentence = sentence.mean(axis=0)\n",
    "        embedding_vectors.append(sentence)\n",
    "    embedding_vectors = np.array(embedding_vectors)\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GLove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove-wiki-gigaword-300\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load glove vectors\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m glove_vectors\u001b[38;5;241m=\u001b[39m\u001b[43mget_vectors_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglove_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m glove_vectors\n",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36mget_vectors_pretrained\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vectors_pretrained\u001b[39m(df, model):\n\u001b[1;32m      3\u001b[0m     embedding_vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m partition \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      5\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m partition\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n",
    "glove_vectors=get_vectors_pretrained(df,glove_model)\n",
    "glove_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fast text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "fast_text_model = api.load(\"fasttext-wiki-news-subwords-300\")  # load glove vectors\n",
    "fast_text_vectors=get_vectors_pretrained(df,fast_text_model)\n",
    "fast_text_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # load glove vectors\n",
    "word2vec_vectors = get_vectors_pretrained(df,word2vec_model)\n",
    "word2vec_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "paragraphs = df[\"text\"].to_list()\n",
    "docs = []\n",
    "\n",
    "for sen in paragraphs:\n",
    "    docs.append(list(sen.split()))\n",
    "print(len(docs))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print(len(corpus[2]))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "#print(len(dictionary))\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = model.get_document_topics(corpus)\n",
    "num_docs = len(all_topics)\n",
    "\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "lda_to_cluster = all_topics_csr.T.toarray()\n",
    "lda_to_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "## Evaluating coherence of gensim LDA model\n",
    "cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "coherence_score = cm.get_coherence()\n",
    "print(coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding={\n",
    "    'BOW':BOW_transformation.toarray(),\n",
    "    'TF_IDF 1_gram':tfidf_1g_transformation.toarray(),\n",
    "    'Doc2vec':np.array(doc2vec_vectors),\n",
    "    'Glove':glove_vectors,\n",
    "    'FastText':fast_text_vectors,\n",
    "    'Word2vec':word2vec_vectors,\n",
    "    'BERT':bert_vectors,\n",
    "    'LDA':lda_to_cluster,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"EmbeddingText_edited.pkl\", \"wb\")\n",
    "pickle.dump(text_embedding, a_file)\n",
    "a_file.close()\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open('EmbeddingText_edited.pkl', 'rb') as f:\n",
    "    text_embedding = pickle.load(f)\n",
    "y_actual=list(pd.read_csv('text-subj.csv')['subjects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2,)\n",
    "embedding=text_embedding.copy()\n",
    "\n",
    "for key in embedding.keys():\n",
    "    # embedding[key]=pca.fit_transform(embedding[key])\n",
    "    embedding[key]=pca.fit_transform(embedding[key])\n",
    "    df=pd.DataFrame({'PCA1':embedding[key][:,0],'PCA2':embedding[key][:,1],'Target':y_actual})\n",
    "    fig = px.scatter(data_frame =df, x='PCA1', y='PCA2', color='Target')\n",
    "    fig.update_layout(title={'text':f'{key}','x':0.5},height=500,width=700)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_w_subj.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_w_subj.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_w_subj.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_w_subj.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e40f2e54c04b87926ee182eaee0794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book was about one of their own: a college student.\n",
      "\n",
      "Ricardo is charged with attempting to overthrow a government, according to the affidavit. He is accused of attempting to overthrow a federal judge, according to the affidavit.\n",
      "\n",
      "The incident occurred in March 2011 when a group of students marched in the student government building. In the protest, dozens of students and students' leaders used their phones to call police.\n",
      "\n",
      "The officers were told to \"shoot\" and \"stop\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('text_subj.csv')\n",
    "\n",
    "# Extract the text column\n",
    "text_data = df['text']\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# Set the device to use (either GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = 'The book was about'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "output_ids = model.generate(input_ids, max_length=100, do_sample=True)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning:\n",
      "\n",
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextGenerationPipeline\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('text_subj.csv')\n",
    "\n",
    "# Extract the text column\n",
    "text_data = df['text']\n",
    "\n",
    "# Load a pre-trained language model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on your specific dataset\n",
    "# ...\n",
    "\n",
    "# Customize the model architecture\n",
    "# ...\n",
    "\n",
    "# Set up a pipeline for text generation\n",
    "generator: TextGenerationPipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = 'The book was about'\n",
    "generated_text = generator(prompt, max_length=100, do_sample=True)[0]['generated_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from en-core-web-sm==2.2.0) (3.3.1)\r\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\r\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.7)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.1.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.6)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.7.7)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.10.1)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.6)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.9.1)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.15)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.65.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (65.6.3)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.28.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.3.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (23.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.7)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.4.6)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.9)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from pathy>=0.3.5->spacy>=2.2.0->en-core-web-sm==2.2.0) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.2.0->en-core-web-sm==2.2.0) (4.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.26.14)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from jinja2->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.1.1)\r\n",
      "Building wheels for collected packages: en-core-web-sm\r\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019105 sha256=c02a4fd6d5fb36b38df694872eaf4580c5bb172c0dcbe2c96443d2f6876f205c\r\n",
      "  Stored in directory: /Users/jeffereyreng/Library/Caches/pip/wheels/f9/7e/12/0c885b1d01a93f5cfff2e269634078c488729f52129c8f7bde\r\n",
      "Successfully built en-core-web-sm\r\n",
      "Installing collected packages: pydantic, en-core-web-sm\r\n",
      "  Attempting uninstall: pydantic\r\n",
      "    Found existing installation: pydantic 1.10.2\r\n",
      "    Uninstalling pydantic-1.10.2:\r\n",
      "      Successfully uninstalled pydantic-1.10.2\r\n",
      "Successfully installed en-core-web-sm-2.2.0 pydantic-1.8.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:845: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.3.1,<3.4.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generator(prompt, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Apply NLP techniques to the generated text\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(generated_text)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Extract named entities from the generated text\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:420\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:453\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:619\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:487\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    485\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m--> 487\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    489\u001b[0m     config, vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, exclude\u001b[38;5;241m=\u001b[39mexclude, meta\u001b[38;5;241m=\u001b[39mmeta\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:650\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mconfig_path, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig file\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_disk(\n\u001b[1;32m    652\u001b[0m         config_path, overrides\u001b[38;5;241m=\u001b[39moverrides, interpolate\u001b[38;5;241m=\u001b[39minterpolate\n\u001b[1;32m    653\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextGenerationPipeline, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('text_subj.csv')\n",
    "\n",
    "# Extract the text column\n",
    "text_data = df['text']\n",
    "\n",
    "# Load a pre-trained language model and tokenizer\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on your specific dataset\n",
    "# ...\n",
    "\n",
    "# Customize the model architecture\n",
    "# ...\n",
    "\n",
    "# Set up a pipeline for text generation\n",
    "generator: TextGenerationPipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = 'The book was about'\n",
    "generated_text = generator(prompt, max_length=100, do_sample=True)[0]['generated_text']\n",
    "\n",
    "# Apply NLP techniques to the generated text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(generated_text)\n",
    "\n",
    "# Extract named entities from the generated text\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Extract parts of speech from the generated text\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# Apply sentiment analysis to the generated text using the BERT model\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model=classification_model, tokenizer=tokenizer)\n",
    "sentiment = sentiment_analyzer(generated_text)[0]\n",
    "print('Sentiment:', sentiment['label'], sentiment['score'])\n",
    "\n",
    "# Generate more text based on the previous output and repeat the process\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('text_subj.csv')\n",
    "\n",
    "# Extract the text column\n",
    "text_data = df['text']\n",
    "\n",
    "# Load a pre-trained language model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on your specific dataset\n",
    "train_texts = text_data.values.tolist()\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = MyDataset(train_encodings)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    warmup_steps=1000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Customize the model architecture\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting textblob\r\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from textblob) (3.7)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.7.9)\r\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\r\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.0.4)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.0)\r\n",
      "Installing collected packages: textblob\r\n",
      "Successfully installed textblob-0.17.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 0.019999999999999997\n",
      "Subjectivity: 0.22000000000000003\n"
     ]
    }
   ],
   "source": [
    "# Apply sentiment analysis to the generated text\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Apply sentiment analysis to the generated text\n",
    "blob = TextBlob(generated_text)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "print('Sentiment:', polarity)\n",
    "print('Subjectivity:', subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Generate more text based on the previous output and repeat the process\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Set up a loop to generate multiple outputs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Generate text based on the previous output\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m generated_text\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:420\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:453\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:619\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:487\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    485\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m--> 487\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    489\u001b[0m     config, vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, exclude\u001b[38;5;241m=\u001b[39mexclude, meta\u001b[38;5;241m=\u001b[39mmeta\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/spacy/util.py:650\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mconfig_path, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig file\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_disk(\n\u001b[1;32m    652\u001b[0m         config_path, overrides\u001b[38;5;241m=\u001b[39moverrides, interpolate\u001b[38;5;241m=\u001b[39minterpolate\n\u001b[1;32m    653\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /opt/homebrew/anaconda3/envs/workspace2/lib/python3.10/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "#Generate more text based on the previous output and repeat the process\n",
    "# Set up a loop to generate multiple outputs\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i in range(3):\n",
    "    # Generate text based on the previous output\n",
    "    prompt = generated_text\n",
    "    generated_text = generator(prompt, max_length=100, do_sample=True)[0]['generated_text']\n",
    "\n",
    "    # Apply NLP techniques to the generated text\n",
    "    doc = nlp(generated_text)\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    parts_of_speech = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "    # Apply sentiment analysis to the generated text\n",
    "    blob = TextBlob(generated_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    # Print out the generated text and its properties\n",
    "    print('Generated text:', generated_text)\n",
    "    print('Named entities:', named_entities)\n",
    "    print('Parts of speech:', parts_of_speech)\n",
    "    print('Sentiment:', polarity)\n",
    "    print('Subjectivity:', subjectivity)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
