{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06926fb",
   "metadata": {
    "endofcell": "--"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} \\\n",
    "         if (device == \"cuda:0\" or device == 'mps') else {}\n",
    "collate_func = lambda x: tuple(x_.to(device) for x_ in default_collate(x)) \\\n",
    "               if device != \"cpu\" else default_collate\n",
    "\n",
    "# with open('embedding_data.pkl', 'rb') as f:\n",
    "#     embed = pkl.load(f)\n",
    "\n",
    "# embed_df = pd.DataFrame(embed)\n",
    "\n",
    "\n",
    "# n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# label_encoder=OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# # -\n",
    "\n",
    "\n",
    "# X = embed_df['vectors'] # Word Embeddings\n",
    "\n",
    "# +\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state =699\n",
    "\n",
    "\n",
    "# +\n",
    "with open('sentence_embed.pkl', 'rb') as f:\n",
    "    embed = pkl.load(f)\n",
    "\n",
    "embed_df = pd.DataFrame(embed)\n",
    "\n",
    "data = U.load_file('data_vFFF.pkl', 'pkl', config['DATADIR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1b5726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f340de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "embed_df = pd.DataFrame(data)\n",
    "\n",
    "embed_df['join_text'] = embed_df['text'].apply(' '.join)\n",
    "embed_df = embed_df[['author_id', 'text', 'join_text', 'passage_key']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "848df17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9deb61e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srtoner/.local/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y= label_encoder.fit_transform(embed_df['author_id'].to_numpy(dtype='int32').reshape(-1,1))\n",
    "X = embed_df['passage_key']\n",
    "\n",
    "X_train, X_test, y_train, y_test = U.train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = U.train_test_split(X_train, y_train, test_size=val_size/(1-test_size),\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f7a77900",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df['label'] = y\n",
    "\n",
    "train_set = set(X_train)\n",
    "val_set = set(X_val)\n",
    "test_set = set(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b5f342c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = embed_df[embed_df['passage_key'].apply(lambda S: S in train_set)]\n",
    "val = embed_df[embed_df['passage_key'].apply(lambda S: S in val_set)]\n",
    "test = embed_df[embed_df['passage_key'].apply(lambda S: S in test_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e42d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bdbd11fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>join_text</th>\n",
       "      <th>passage_key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "      <td>[Croft’s next words explained it to be Mr Went...</td>\n",
       "      <td>Croft’s next words explained it to be Mr Wentw...</td>\n",
       "      <td>PG105_2371</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5049</td>\n",
       "      <td>[name and livery included; but I will not pret...</td>\n",
       "      <td>name and livery included; but I will not prete...</td>\n",
       "      <td>PG105_7471</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5049</td>\n",
       "      <td>[, “Ah! Miss Anne, if it had pleased Heaven to...</td>\n",
       "      <td>“Ah! Miss Anne, if it had pleased Heaven to s...</td>\n",
       "      <td>PG105_2871</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5049</td>\n",
       "      <td>[fairly off, therefore, before she began to ta...</td>\n",
       "      <td>fairly off, therefore, before she began to tal...</td>\n",
       "      <td>PG105_7921</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5049</td>\n",
       "      <td>[, This decision checked Mary’s eagerness, and...</td>\n",
       "      <td>This decision checked Mary’s eagerness, and s...</td>\n",
       "      <td>PG105_5121</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78583</th>\n",
       "      <td>22568</td>\n",
       "      <td>[from whom, do you think? From Lilly Larnard, ...</td>\n",
       "      <td>from whom, do you think? From Lilly Larnard, t...</td>\n",
       "      <td>PG53940_3241</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78585</th>\n",
       "      <td>22568</td>\n",
       "      <td>[the upper one, somewhat after the manner of t...</td>\n",
       "      <td>the upper one, somewhat after the manner of th...</td>\n",
       "      <td>PG53940_4091</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78587</th>\n",
       "      <td>22568</td>\n",
       "      <td>[And such a supper! Our host presided, and whi...</td>\n",
       "      <td>And such a supper! Our host presided, and whil...</td>\n",
       "      <td>PG53940_2441</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78588</th>\n",
       "      <td>22568</td>\n",
       "      <td>[himself, and you will lose him. Keep him in s...</td>\n",
       "      <td>himself, and you will lose him. Keep him in sl...</td>\n",
       "      <td>PG53940_4241</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78589</th>\n",
       "      <td>22568</td>\n",
       "      <td>[regularity. The tiny streams among the hills ...</td>\n",
       "      <td>regularity. The tiny streams among the hills a...</td>\n",
       "      <td>PG53940_7991</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47154 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                               text  \\\n",
       "0           5049  [Croft’s next words explained it to be Mr Went...   \n",
       "1           5049  [name and livery included; but I will not pret...   \n",
       "3           5049  [, “Ah! Miss Anne, if it had pleased Heaven to...   \n",
       "4           5049  [fairly off, therefore, before she began to ta...   \n",
       "5           5049  [, This decision checked Mary’s eagerness, and...   \n",
       "...          ...                                                ...   \n",
       "78583      22568  [from whom, do you think? From Lilly Larnard, ...   \n",
       "78585      22568  [the upper one, somewhat after the manner of t...   \n",
       "78587      22568  [And such a supper! Our host presided, and whi...   \n",
       "78588      22568  [himself, and you will lose him. Keep him in s...   \n",
       "78589      22568  [regularity. The tiny streams among the hills ...   \n",
       "\n",
       "                                               join_text   passage_key  label  \n",
       "0      Croft’s next words explained it to be Mr Wentw...    PG105_2371    115  \n",
       "1      name and livery included; but I will not prete...    PG105_7471    115  \n",
       "3       “Ah! Miss Anne, if it had pleased Heaven to s...    PG105_2871    115  \n",
       "4      fairly off, therefore, before she began to tal...    PG105_7921    115  \n",
       "5       This decision checked Mary’s eagerness, and s...    PG105_5121    115  \n",
       "...                                                  ...           ...    ...  \n",
       "78583  from whom, do you think? From Lilly Larnard, t...  PG53940_3241    180  \n",
       "78585  the upper one, somewhat after the manner of th...  PG53940_4091    180  \n",
       "78587  And such a supper! Our host presided, and whil...  PG53940_2441    180  \n",
       "78588  himself, and you will lose him. Keep him in sl...  PG53940_4241    180  \n",
       "78589  regularity. The tiny streams among the hills a...  PG53940_7991    180  \n",
       "\n",
       "[47154 rows x 5 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cf767a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fac1355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/srtoner/.cache/huggingface/datasets/csv/default-bff310e37cdca341/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7b7de87f7448e5ae9bc8491dfe8557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530cb8da2794aa8a53dfee680ce7fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/srtoner/.cache/huggingface/datasets/csv/default-bff310e37cdca341/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/srtoner/.cache/huggingface/datasets/csv/default-a3cfc9b9446bcb41/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbd83a9275e4b76958db1efe34c12ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551fcfdaa044ec682f07cf0c1c001d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/srtoner/.cache/huggingface/datasets/csv/default-a3cfc9b9446bcb41/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/srtoner/.cache/huggingface/datasets/csv/default-4ead8133ebe384b5/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558fd36e120f4278a43b5e6a8172cfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeacf5e6a594a26a5227db9ec260e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/srtoner/.cache/huggingface/datasets/csv/default-4ead8133ebe384b5/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "ds = {\n",
    "      'train' :  Dataset.from_csv('train.csv'),\n",
    "      'val' :  Dataset.from_csv('val.csv'),\n",
    "      'test' :  Dataset.from_csv('test.csv'),\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9084a4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_id', 'text', 'join_text', 'passage_key', 'label'], dtype='object')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2ef5fc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>join_text</th>\n",
       "      <th>passage_key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5049</td>\n",
       "      <td>[Croft’s next words explained it to be Mr Went...</td>\n",
       "      <td>Croft’s next words explained it to be Mr Wentw...</td>\n",
       "      <td>PG105_2371</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5049</td>\n",
       "      <td>[name and livery included; but I will not pret...</td>\n",
       "      <td>name and livery included; but I will not prete...</td>\n",
       "      <td>PG105_7471</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5049</td>\n",
       "      <td>[, “Ah! Miss Anne, if it had pleased Heaven to...</td>\n",
       "      <td>“Ah! Miss Anne, if it had pleased Heaven to s...</td>\n",
       "      <td>PG105_2871</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5049</td>\n",
       "      <td>[fairly off, therefore, before she began to ta...</td>\n",
       "      <td>fairly off, therefore, before she began to tal...</td>\n",
       "      <td>PG105_7921</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5049</td>\n",
       "      <td>[, This decision checked Mary’s eagerness, and...</td>\n",
       "      <td>This decision checked Mary’s eagerness, and s...</td>\n",
       "      <td>PG105_5121</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78583</th>\n",
       "      <td>22568</td>\n",
       "      <td>[from whom, do you think? From Lilly Larnard, ...</td>\n",
       "      <td>from whom, do you think? From Lilly Larnard, t...</td>\n",
       "      <td>PG53940_3241</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78585</th>\n",
       "      <td>22568</td>\n",
       "      <td>[the upper one, somewhat after the manner of t...</td>\n",
       "      <td>the upper one, somewhat after the manner of th...</td>\n",
       "      <td>PG53940_4091</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78587</th>\n",
       "      <td>22568</td>\n",
       "      <td>[And such a supper! Our host presided, and whi...</td>\n",
       "      <td>And such a supper! Our host presided, and whil...</td>\n",
       "      <td>PG53940_2441</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78588</th>\n",
       "      <td>22568</td>\n",
       "      <td>[himself, and you will lose him. Keep him in s...</td>\n",
       "      <td>himself, and you will lose him. Keep him in sl...</td>\n",
       "      <td>PG53940_4241</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78589</th>\n",
       "      <td>22568</td>\n",
       "      <td>[regularity. The tiny streams among the hills ...</td>\n",
       "      <td>regularity. The tiny streams among the hills a...</td>\n",
       "      <td>PG53940_7991</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47154 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_id                                               text  \\\n",
       "0           5049  [Croft’s next words explained it to be Mr Went...   \n",
       "1           5049  [name and livery included; but I will not pret...   \n",
       "3           5049  [, “Ah! Miss Anne, if it had pleased Heaven to...   \n",
       "4           5049  [fairly off, therefore, before she began to ta...   \n",
       "5           5049  [, This decision checked Mary’s eagerness, and...   \n",
       "...          ...                                                ...   \n",
       "78583      22568  [from whom, do you think? From Lilly Larnard, ...   \n",
       "78585      22568  [the upper one, somewhat after the manner of t...   \n",
       "78587      22568  [And such a supper! Our host presided, and whi...   \n",
       "78588      22568  [himself, and you will lose him. Keep him in s...   \n",
       "78589      22568  [regularity. The tiny streams among the hills ...   \n",
       "\n",
       "                                               join_text   passage_key  label  \n",
       "0      Croft’s next words explained it to be Mr Wentw...    PG105_2371    115  \n",
       "1      name and livery included; but I will not prete...    PG105_7471    115  \n",
       "3       “Ah! Miss Anne, if it had pleased Heaven to s...    PG105_2871    115  \n",
       "4      fairly off, therefore, before she began to tal...    PG105_7921    115  \n",
       "5       This decision checked Mary’s eagerness, and s...    PG105_5121    115  \n",
       "...                                                  ...           ...    ...  \n",
       "78583  from whom, do you think? From Lilly Larnard, t...  PG53940_3241    180  \n",
       "78585  the upper one, somewhat after the manner of th...  PG53940_4091    180  \n",
       "78587  And such a supper! Our host presided, and whil...  PG53940_2441    180  \n",
       "78588  himself, and you will lose him. Keep him in sl...  PG53940_4241    180  \n",
       "78589  regularity. The tiny streams among the hills a...  PG53940_7991    180  \n",
       "\n",
       "[47154 rows x 5 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c02a6802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 47154\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 15718\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 15718\n",
       " })}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "55111984",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "ds1 = {}\n",
    "\n",
    "BASE_MODEL = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "# BASE_MODEL = \"allenai/longformer-base-4096\"\n",
    "# BASE_MODEL = \"lreN5bs16\" # Learning Rate 2e-5, batch size 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = .1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, \n",
    "                                                           num_labels=n_classes,\n",
    "                                                           ignore_mismatched_sizes=True)\n",
    "\n",
    "def preprocess_function(examples, test = False):\n",
    "    # if not test:\n",
    "    label = examples[\"label\"] \n",
    "    examples = tokenizer(examples[\"join_text\"],\n",
    "                        truncation=True, \n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                        return_tensors='pt')\n",
    "    \n",
    "    for key in examples:\n",
    "        examples[key] = examples[key].squeeze(0)\n",
    "  \n",
    "    # if not test:\n",
    "    examples[\"label\"] = torch.IntTensor([label])\n",
    "    examples = examples.to(device)\n",
    "    return examples\n",
    "\n",
    "for split in ds:\n",
    "    ds1[split] = ds[split].map(preprocess_function, \n",
    "                                remove_columns=['author_id', 'text', 'passage_key','join_text', 'label'])\n",
    "\n",
    "    ds1[split].set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9b47b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps = 1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    report_to = 'tensorboard'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(1, 0.01)\n",
    "tb = TensorBoardCallback()\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.functional.cross_entropy\n",
    "        loss = loss_fct(logits.view(-1, n_classes), labels.view(-1), average = 'weighted')\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cffcf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics_for_classification(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(labels)\n",
    "    print(type(predictions))\n",
    "    \n",
    "    predicted_class = predictions.argmax(axis=1)\n",
    "    print(predicted_class)\n",
    "    f1 = f1_score(labels, predicted_class)\n",
    "    print(f\"F1: {f1}\")\n",
    "    \n",
    "    return {\"F1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f35b3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds1[\"train\"],\n",
    "    eval_dataset=ds1[\"val\"],\n",
    "    compute_metrics=compute_metrics_for_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fc9fac36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srtoner/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 15.78 GiB total capacity; 14.54 GiB already allocated; 70.69 MiB free; 14.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2001299/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2001299/30497443.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1936\u001b[0m             \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m         outputs = self.longformer(\n\u001b[0m\u001b[1;32m   1939\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         )\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 )\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     ):\n\u001b[0;32m-> 1249\u001b[0;31m         self_attn_outputs = self.attention(\n\u001b[0m\u001b[1;32m   1250\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     ):\n\u001b[0;32m-> 1185\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_global_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# compute sum of global and local attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             attn_output = self._compute_attn_output_with_global_indices(\n\u001b[0m\u001b[1;32m    655\u001b[0m                 \u001b[0mvalue_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0mattn_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_compute_attn_output_with_global_indices\u001b[0;34m(self, value_vectors, attn_probs, max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;31m# compute attn output with global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0mattn_probs_without_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_matmul_attn_probs_value\u001b[0;34m(self, attn_probs, value, window_overlap)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;31m# group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\n\u001b[0m\u001b[1;32m    903\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_overlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounding_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trunc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 15.78 GiB total capacity; 14.54 GiB already allocated; 70.69 MiB free; 14.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e38d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c23495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7c428",
   "metadata": {},
   "source": [
    "prediction = trainer.predict(ds['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e66c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('hf_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
