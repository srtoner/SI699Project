{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790d3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} \\\n",
    "         if (device == \"cuda:0\" or device == 'mps') else {}\n",
    "collate_func = lambda x: tuple(x_.to(device) for x_ in default_collate(x)) \\\n",
    "               if device != \"cpu\" else default_collate\n",
    "\n",
    "with open('embedding_data.pkl', 'rb') as f:\n",
    "    embed = pkl.load(f)\n",
    "\n",
    "embed_df = pd.DataFrame(embed)\n",
    "\n",
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder=OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# -\n",
    "\n",
    "\n",
    "# X = embed_df['vectors'] # Word Embeddings\n",
    "\n",
    "# +\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state =699\n",
    "\n",
    "\n",
    "# +\n",
    "# with open('embedding_data.pkl', 'rb') as f:\n",
    "#     embed = pkl.load(f)\n",
    "\n",
    "# embed_df = pd.DataFrame(embed)\n",
    "\n",
    "data = U.load_file('data_vFF.pkl', 'pkl', config['DATADIR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bfdc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_df = pd.DataFrame(data)\n",
    "\n",
    "embed_df['join_text'] = embed_df['text'].apply(' '.join)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d4c469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephentoner/miniconda3/envs/si699proj/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y= label_encoder.fit_transform(embed_df['author_id'].to_numpy(dtype='int32').reshape(-1,1))\n",
    "X = embed_df['passage_key']\n",
    "\n",
    "X_train, X_test, y_train, y_test = U.train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = U.train_test_split(X_train, y_train, test_size=val_size/(1-test_size),\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fe16ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df['label'] = y\n",
    "\n",
    "train_set = set(X_train)\n",
    "val_set = set(X_val)\n",
    "test_set = set(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "414b0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = embed_df[embed_df['passage_key'].apply(lambda S: S in train_set)]\n",
    "val = embed_df[embed_df['passage_key'].apply(lambda S: S in val_set)]\n",
    "test = embed_df[embed_df['passage_key'].apply(lambda S: S in test_set)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a1a845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_id</th>\n",
       "      <th>gutenbergbookid</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lines</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>Sub_A</th>\n",
       "      <th>Sub_B</th>\n",
       "      <th>Sub_C</th>\n",
       "      <th>str_text_lines</th>\n",
       "      <th>passage_key</th>\n",
       "      <th>join_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author_id, author_name, book_id, gutenbergbookid, title, text, text_lines, authoryearofbirth, authoryearofdeath, downloads, subjects, topic, Sub_A, Sub_B, Sub_C, str_text_lines, passage_key, join_text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a61697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68990528",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {\n",
    "      'train' :  Dataset.from_dict(train.to_dict()),\n",
    "      'val' :  Dataset.from_dict(val.to_dict()),\n",
    "      'test' :  Dataset.from_dict(test.to_dict()),\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1b62753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_id', 'author_name', 'book_id', 'gutenbergbookid', 'title',\n",
       "       'text', 'text_lines', 'authoryearofbirth', 'authoryearofdeath',\n",
       "       'downloads', 'subjects', 'topic', 'Sub_A', 'Sub_B', 'Sub_C',\n",
       "       'str_text_lines', 'passage_key', 'join_text', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11f7a9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed_df.join_text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "558e6948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                      \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m ds:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m         ds[split] \u001b[39m=\u001b[39m ds[split]\u001b[39m.\u001b[39;49mmap(preprocess_function, \n\u001b[1;32m     37\u001b[0m                                 remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mauthor_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mauthor_name\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbook_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgutenbergbookid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m        \u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtext_lines\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mauthoryearofbirth\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mauthoryearofdeath\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m        \u001b[39m'\u001b[39;49m\u001b[39mdownloads\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msubjects\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSub_A\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSub_B\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSub_C\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m        \u001b[39m'\u001b[39;49m\u001b[39mstr_text_lines\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpassage_key\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mjoin_text\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     41\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         ds[split] \u001b[39m=\u001b[39m ds[split]\u001b[39m.\u001b[39mmap(preprocess_function, \n\u001b[1;32m     43\u001b[0m                                 remove_columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mauthor_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthor_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbook_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgutenbergbookid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtext_lines\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthoryearofbirth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthoryearofdeath\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     45\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdownloads\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubjects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSub_A\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSub_B\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSub_C\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     46\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mstr_text_lines\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpassage_key\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mjoin_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m],fn_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m})\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/datasets/arrow_dataset.py:3004\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   2998\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2999\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3003\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3004\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3005\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3006\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/datasets/arrow_dataset.py:3358\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3356\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3357\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3358\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3359\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3360\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/datasets/arrow_dataset.py:3261\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3260\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3261\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3263\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3264\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3265\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[48], line 21\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(examples, test)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples, test \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# if not test:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     label \u001b[39m=\u001b[39m examples[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \n\u001b[0;32m---> 21\u001b[0m     examples \u001b[39m=\u001b[39m tokenizer(examples[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     22\u001b[0m                         truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     23\u001b[0m                         padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m                         max_length\u001b[39m=\u001b[39;49mMAX_LENGTH,\n\u001b[1;32m     25\u001b[0m                         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m examples:\n\u001b[1;32m     28\u001b[0m         examples[key] \u001b[39m=\u001b[39m examples[key]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2529\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2530\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2588\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2588\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2589\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2590\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2593\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2594\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2595\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2596\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2597\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BASE_MODEL = \"allenai/longformer-base-4096\"\n",
    "# BASE_MODEL = \"lreN5bs16\" # Learning Rate 2e-5, batch size 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 400\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, \n",
    "                                                           num_labels=n_classes,\n",
    "                                                           ignore_mismatched_sizes=True)\n",
    "\n",
    "def preprocess_function(examples, test = False):\n",
    "    # if not test:\n",
    "    label = examples[\"label\"] \n",
    "    examples = tokenizer(examples[\"text\"],\n",
    "                        truncation=True, \n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                        return_tensors='pt')\n",
    "    \n",
    "    for key in examples:\n",
    "        examples[key] = examples[key].squeeze(0)\n",
    "    # if not test:\n",
    "    examples[\"label\"] = torch.FloatTensor([label])\n",
    "    examples = examples.to(device)\n",
    "    return examples\n",
    "\n",
    "for split in ds:\n",
    "    if split != 'test':\n",
    "        ds[split] = ds[split].map(preprocess_function, \n",
    "                                remove_columns=['author_id', 'author_name', 'book_id', 'gutenbergbookid', 'title',\n",
    "       'text', 'text_lines', 'authoryearofbirth', 'authoryearofdeath',\n",
    "       'downloads', 'subjects', 'topic', 'Sub_A', 'Sub_B', 'Sub_C',\n",
    "       'str_text_lines', 'passage_key','join_text', 'label'])\n",
    "    else:\n",
    "        ds[split] = ds[split].map(preprocess_function, \n",
    "                                remove_columns=['author_id', 'author_name', 'book_id', 'gutenbergbookid', 'title',\n",
    "       'text', 'text_lines', 'authoryearofbirth', 'authoryearofdeath',\n",
    "       'downloads', 'subjects', 'topic', 'Sub_A', 'Sub_B', 'Sub_C',\n",
    "       'str_text_lines', 'passage_key','join_text', 'label'],fn_kwargs = {'test':True})\n",
    "\n",
    "    ds[split].set_format('pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "848673db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps = 1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    report_to = 'tensorboard'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(1, 0.01)\n",
    "tb = TensorBoardCallback()\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a128f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics_for_classification(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    f1 = f1_score(labels, predictions, squared=True)\n",
    "    print(f\"F1: {f1}\")\n",
    "    \n",
    "    return {\"F1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601b8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"val\"],\n",
    "    compute_metrics=compute_metrics_for_classification,\n",
    "    callbacks=[tb, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a518d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: [],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86ecd3d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m   1644\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[1;32m   1647\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtrain_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:881\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    865\u001b[0m         train_dataset \u001b[39m=\u001b[39m IterableDatasetShard(\n\u001b[1;32m    866\u001b[0m             train_dataset,\n\u001b[1;32m    867\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m             process_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mprocess_index,\n\u001b[1;32m    871\u001b[0m         )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    874\u001b[0m         train_dataset,\n\u001b[1;32m    875\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         pin_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_pin_memory,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[0;32m--> 881\u001b[0m train_sampler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_train_sampler()\n\u001b[1;32m    883\u001b[0m \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    884\u001b[0m     train_dataset,\n\u001b[1;32m    885\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m     worker_init_fn\u001b[39m=\u001b[39mseed_worker,\n\u001b[1;32m    892\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:823\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 823\u001b[0m         \u001b[39mreturn\u001b[39;00m RandomSampler(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataset, generator\u001b[39m=\u001b[39;49mgenerator)\n\u001b[1;32m    824\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    825\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39min\u001b[39;00m [ParallelMode\u001b[39m.\u001b[39mTPU, ParallelMode\u001b[39m.\u001b[39mSAGEMAKER_MODEL_PARALLEL]\n\u001b[1;32m    826\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_drop_last\n\u001b[1;32m    827\u001b[0m     ):\n\u001b[1;32m    828\u001b[0m         \u001b[39m# Use a loop for TPUs when drop_last is False to have all batches have the same size.\u001b[39;00m\n\u001b[1;32m    829\u001b[0m         \u001b[39mreturn\u001b[39;00m DistributedSamplerWithLoop(\n\u001b[1;32m    830\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset,\n\u001b[1;32m    831\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mper_device_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m             seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    835\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92d7ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "embedding_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a22d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "trained_weights = embedding_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAttentionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, hidden_dim,trained_weights, n_classes):\n",
    "        '''\n",
    "        Creates the new classifier model. embeddings_fname is a string containing the\n",
    "        filename with the saved pytorch parameters (the state dict) for the Embedding\n",
    "        object that should be used to initialize this class's word Embedding parameters\n",
    "        '''\n",
    "        super(DocumentAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Save the input arguments to the state\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embeddings_fname = vocab_size        \n",
    "        \n",
    "       \n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, n_classes)\n",
    "        # trained_weights = torch.load(embeddings_fname)['target_embeddings.weight']\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(trained_weights, freeze = False)\n",
    "        # # self.embeddings = nn.Embedding()\n",
    "\n",
    "        # self.lstm = nn.LSTM(embedding_size, hidden_dim, bidirectional=True)\n",
    "        # # self.attention = torch.rand(self.num_heads, self.embedding_size, requires_grad = True, device=device)\n",
    "        # self.attention = torch.rand(self.num_heads, hidden_dim * 2, requires_grad = True, device=device)\n",
    "        # self.linear = nn.Linear(num_heads * embedding_size, n_classes)\n",
    "    \n",
    "    def forward(self, w):\n",
    "        embedded = self.embeddings(w)\n",
    "        lstm_output, _ = self.lstm(w.transpose(1,2)) \n",
    "        attention_scores = self.attention(lstm_output) \n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  \n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)  \n",
    "        output = self.fc(context_vector)  # shape: (batch_size, num_classes)\n",
    "        # w = w.squeeze()\n",
    "\n",
    "        # lstm_out, _ = self.lstm(w.T)\n",
    "        # # w = torch.t(self.embeddings(word_ids).squeeze()) # Embedding_Dim \n",
    "        # r = torch.matmul(self.attention, lstm_out.T)\n",
    "        # a = torch.softmax(r, 1)\n",
    "        # reweighted = a @ w.T\n",
    "        # output = self.linear(reweighted.view(-1))\n",
    "\n",
    "        # return torch.softmax(output, dim=0), a.T\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b249bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "datasets['train'] = list(zip(X_train, y_train))\n",
    "datasets['val'] = list(zip(X_val, y_val))\n",
    "datasets['test'] = list(zip(X_test, y_test))\n",
    "\n",
    "train_list = datasets['train']\n",
    "val_list = datasets['val']\n",
    "\n",
    "model = DocumentAttentionClassifier(1, 50, 4, 32, trained_weights, n_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16750726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, eval_data, n_classes, kwargs):\n",
    "    '''\n",
    "    Scores the model on the evaluation data and returns the F1\n",
    "    Eval Data must be in DataLoader-ready format\n",
    "    '''\n",
    "\n",
    "    eval_loader = DataLoader(eval_data, batch_size = 1, shuffle = False, collate_fn=collate_func, **kwargs)\n",
    "\n",
    "    threshold = 0.2\n",
    "    probs = np.zeros((len(eval_loader), n_classes))\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, x in enumerate(eval_loader):\n",
    "            word_ids, label = x\n",
    "            labels.append(label.cpu().numpy())\n",
    "            output, weights = model(word_ids)\n",
    "            probs[idx] = output.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array([np.argmax(p) for p in probs], dtype = int)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    y_true = [np.argmax(l) for l in labels]\n",
    "    \n",
    "    \n",
    "    return labels, y_pred, f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791361d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_period = 5\n",
    "# model = model.to(device)\n",
    "writer = SummaryWriter()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# VVV GOLD STANDARD VVV\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.1)\n",
    "# ^^^ GOLD STANDARD ^^^\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr = 5e-3, weight_decay = 0.1)\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters())\n",
    "# optimizer = optim.RMSprop(model.parameters(), 5e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 5e-4)\n",
    "\n",
    "train_loader = DataLoader(train_list, batch_size=16, shuffle=True, collate_fn=collate_func, **kwargs)\n",
    "n_epochs = 10\n",
    "# n_epochs = 1\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}\n",
    "loss_idx = 0\n",
    "loss_record = []\n",
    "model.train()\n",
    "\n",
    "# + vscode={\"languageId\": \"python\"}r\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    for step, data in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        word_ids, labels = data\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        output, weights = model(word_ids)\n",
    "        loss = loss_function(output, labels.squeeze().float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to tensorboard. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "\n",
    "        if not step % loss_period and step:\n",
    "            writer.add_scalar(\"Loss\", loss_sum, loss_idx)\n",
    "            if not step % (loss_period * 10) and step:\n",
    "                model.eval()\n",
    "                _y, _y2, f1 = run_eval(model, val_list, n_classes, kwargs)\n",
    "                writer.add_scalar(\"F1\", f1, loss_idx)\n",
    "                model.train()\n",
    "            loss_record.append(loss_sum)\n",
    "            loss_sum = 0\n",
    "            loss_idx += 1\n",
    "            \n",
    "\n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        \n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()\n",
    "\n",
    "torch.save(optimizer.state_dict(), 'trained_opt_')\n",
    "torch.save(model.state_dict(), 'trained_model_')\n",
    "\n",
    "y_true, y_pred, f1 = run_eval(model, val_list, n_classes, kwargs)\n",
    "print(\"F1 Score of : \"+ str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ed858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si699proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
