{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790d3b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephentoner/miniconda3/envs/si699proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(config['REPODIR'])\n",
    "import Utils as U\n",
    "from Corpus import Corpus\n",
    "os.chdir(cwd)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"dark\")\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} \\\n",
    "         if (device == \"cuda:0\" or device == 'mps') else {}\n",
    "collate_func = lambda x: tuple(x_.to(device) for x_ in default_collate(x)) \\\n",
    "               if device != \"cpu\" else default_collate\n",
    "\n",
    "with open('embedding_data.pkl', 'rb') as f:\n",
    "    embed = pkl.load(f)\n",
    "\n",
    "embed_df = pd.DataFrame(embed)\n",
    "\n",
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder=OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# -\n",
    "\n",
    "\n",
    "# X = embed_df['vectors'] # Word Embeddings\n",
    "\n",
    "# +\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "random_state =699\n",
    "\n",
    "\n",
    "# +\n",
    "# with open('embedding_data.pkl', 'rb') as f:\n",
    "#     embed = pkl.load(f)\n",
    "\n",
    "# embed_df = pd.DataFrame(embed)\n",
    "\n",
    "data = U.load_file('data_vFFF.pkl', 'pkl', config['DATADIR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfdc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_df = pd.DataFrame(data)\n",
    "\n",
    "embed_df['join_text'] = embed_df['text'].apply(' '.join)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4c469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephentoner/miniconda3/envs/si699proj/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_classes = embed_df.author_id.nunique()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y= label_encoder.fit_transform(embed_df['author_id'].to_numpy(dtype='int32').reshape(-1,1))\n",
    "X = embed_df['passage_key']\n",
    "\n",
    "X_train, X_test, y_train, y_test = U.train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = U.train_test_split(X_train, y_train, test_size=val_size/(1-test_size),\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe16ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df['label'] = y\n",
    "\n",
    "train_set = set(X_train)\n",
    "val_set = set(X_val)\n",
    "test_set = set(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414b0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = embed_df[embed_df['passage_key'].apply(lambda S: S in train_set)]\n",
    "val = embed_df[embed_df['passage_key'].apply(lambda S: S in val_set)]\n",
    "test = embed_df[embed_df['passage_key'].apply(lambda S: S in test_set)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1a845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/stephentoner/.cache/huggingface/datasets/csv/default-18e5f7dd8ddcf452/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 183.35it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/stephentoner/.cache/huggingface/datasets/csv/default-18e5f7dd8ddcf452/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/stephentoner/.cache/huggingface/datasets/csv/default-25cce4c33a48d362/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6502.80it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 658.76it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/stephentoner/.cache/huggingface/datasets/csv/default-25cce4c33a48d362/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /Users/stephentoner/.cache/huggingface/datasets/csv/default-d55677bb2f42207d/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2045.00it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 281.31it/s]\n",
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/stephentoner/.cache/huggingface/datasets/csv/default-d55677bb2f42207d/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "train\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ds = {\n",
    "      'train' :  Dataset.from_csv('train.csv'),\n",
    "      'val' :  Dataset.from_csv('val.csv'),\n",
    "      'test' :  Dataset.from_csv('test.csv'),\n",
    "      }\n",
    "\n",
    "embed_df.columns\n",
    "\n",
    "type(embed_df.join_text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558e6948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BASE_MODEL = \"allenai/longformer-base-4096\"\n",
    "# BASE_MODEL = \"lreN5bs16\" # Learning Rate 2e-5, batch size 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 400\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, \n",
    "                                                           num_labels=n_classes,\n",
    "                                                           ignore_mismatched_sizes=True)\n",
    "\n",
    "def preprocess_function(examples, test = False):\n",
    "    # if not test:\n",
    "    label = examples[\"label\"] \n",
    "    examples = tokenizer(examples[\"text\"],\n",
    "                        truncation=True, \n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                        return_tensors='pt')\n",
    "    \n",
    "    for key in examples:\n",
    "        examples[key] = examples[key].squeeze(0)\n",
    "    # if not test:\n",
    "    examples[\"label\"] = torch.FloatTensor([label])\n",
    "    examples = examples.to(device)\n",
    "    return examples\n",
    "\n",
    "for split in ds:\n",
    "    if split != 'test':\n",
    "        ds[split] = ds[split].map(preprocess_function, \n",
    "                                remove_columns=['author_id', 'author_name', 'book_id', 'gutenbergbookid', 'title',\n",
    "       'text', 'text_lines', 'authoryearofbirth', 'authoryearofdeath',\n",
    "       'downloads', 'subjects', 'topic', 'Sub_A', 'Sub_B', 'Sub_C',\n",
    "       'str_text_lines', 'passage_key','join_text', 'label'])\n",
    "    else:\n",
    "        ds[split] = ds[split].map(preprocess_function, \n",
    "                                remove_columns=['author_id', 'author_name', 'book_id', 'gutenbergbookid', 'title',\n",
    "       'text', 'text_lines', 'authoryearofbirth', 'authoryearofdeath',\n",
    "       'downloads', 'subjects', 'topic', 'Sub_A', 'Sub_B', 'Sub_C',\n",
    "       'str_text_lines', 'passage_key','join_text', 'label'],fn_kwargs = {'test':True})\n",
    "\n",
    "    ds[split].set_format('pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "848673db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps = 1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    report_to = 'tensorboard'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(1, 0.01)\n",
    "tb = TensorBoardCallback()\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a128f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics_for_classification(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    f1 = f1_score(labels, predictions, squared=True)\n",
    "    print(f\"F1: {f1}\")\n",
    "    \n",
    "    return {\"F1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601b8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"val\"],\n",
    "    compute_metrics=compute_metrics_for_classification,\n",
    "    callbacks=[tb, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a518d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: [],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86ecd3d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m   1644\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[1;32m   1647\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtrain_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:881\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    865\u001b[0m         train_dataset \u001b[39m=\u001b[39m IterableDatasetShard(\n\u001b[1;32m    866\u001b[0m             train_dataset,\n\u001b[1;32m    867\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m             process_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mprocess_index,\n\u001b[1;32m    871\u001b[0m         )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    874\u001b[0m         train_dataset,\n\u001b[1;32m    875\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         pin_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_pin_memory,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[0;32m--> 881\u001b[0m train_sampler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_train_sampler()\n\u001b[1;32m    883\u001b[0m \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m    884\u001b[0m     train_dataset,\n\u001b[1;32m    885\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m     worker_init_fn\u001b[39m=\u001b[39mseed_worker,\n\u001b[1;32m    892\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/transformers/trainer.py:823\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 823\u001b[0m         \u001b[39mreturn\u001b[39;00m RandomSampler(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataset, generator\u001b[39m=\u001b[39;49mgenerator)\n\u001b[1;32m    824\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    825\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39min\u001b[39;00m [ParallelMode\u001b[39m.\u001b[39mTPU, ParallelMode\u001b[39m.\u001b[39mSAGEMAKER_MODEL_PARALLEL]\n\u001b[1;32m    826\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_drop_last\n\u001b[1;32m    827\u001b[0m     ):\n\u001b[1;32m    828\u001b[0m         \u001b[39m# Use a loop for TPUs when drop_last is False to have all batches have the same size.\u001b[39;00m\n\u001b[1;32m    829\u001b[0m         \u001b[39mreturn\u001b[39;00m DistributedSamplerWithLoop(\n\u001b[1;32m    830\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset,\n\u001b[1;32m    831\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mper_device_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m             seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    835\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/si699proj/lib/python3.9/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si699proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
