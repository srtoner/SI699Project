{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:11:50.626885Z",
     "end_time": "2023-04-06T12:11:51.379208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  \\\n110438  ['   Stands the castle of the maid;', 'The hou...   \n28916   ['\"Oh, Mr Gresham, such good friends as you an...   \n12917   ['', 'PROSPERO.', 'Ay, with a twink.', '', 'AR...   \n25817   ['Widow Rouleau.”', '', '“Ah! you know the poo...   \n43411   ['feel drawn to Moscow. So far I have not had ...   \n...                                                   ...   \n119584  [\"don't want ... but I don't have to go on. Th...   \n92280   ['a wound all chance of escape would be at an ...   \n140633  ['though!  I expect the fellow we saw was the ...   \n161626  ['all his fellows that I met, he was desperate...   \n78287   ['     _Which Contains Some Rather Painful Exp...   \n\n                                                 subjects  \n110438                              {'Poetry', 'Ballads'}  \n28916   {'Physicians -- Fiction', 'Barsetshire (Englan...  \n12917   {'Fathers and daughters -- Drama', 'Political ...  \n25817                   {'Detective and mystery stories'}  \n43411   {'Chekhov, Anton Pavlovich, 1860-1904 -- Corre...  \n...                                                   ...  \n119584  {'Assassins -- Fiction', 'Short stories', 'Sci...  \n92280     {'Fiction', 'Sea stories', 'Adventure stories'}  \n140633  {'Detective and mystery stories', 'British -- ...  \n161626  {'World War, 1914-1918', 'Great Britain. Army ...  \n78287   {'Aristocracy (Social class) -- Fiction', 'Dan...  \n\n[300 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>subjects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>110438</th>\n      <td>['   Stands the castle of the maid;', 'The hou...</td>\n      <td>{'Poetry', 'Ballads'}</td>\n    </tr>\n    <tr>\n      <th>28916</th>\n      <td>['\"Oh, Mr Gresham, such good friends as you an...</td>\n      <td>{'Physicians -- Fiction', 'Barsetshire (Englan...</td>\n    </tr>\n    <tr>\n      <th>12917</th>\n      <td>['', 'PROSPERO.', 'Ay, with a twink.', '', 'AR...</td>\n      <td>{'Fathers and daughters -- Drama', 'Political ...</td>\n    </tr>\n    <tr>\n      <th>25817</th>\n      <td>['Widow Rouleau.”', '', '“Ah! you know the poo...</td>\n      <td>{'Detective and mystery stories'}</td>\n    </tr>\n    <tr>\n      <th>43411</th>\n      <td>['feel drawn to Moscow. So far I have not had ...</td>\n      <td>{'Chekhov, Anton Pavlovich, 1860-1904 -- Corre...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>119584</th>\n      <td>[\"don't want ... but I don't have to go on. Th...</td>\n      <td>{'Assassins -- Fiction', 'Short stories', 'Sci...</td>\n    </tr>\n    <tr>\n      <th>92280</th>\n      <td>['a wound all chance of escape would be at an ...</td>\n      <td>{'Fiction', 'Sea stories', 'Adventure stories'}</td>\n    </tr>\n    <tr>\n      <th>140633</th>\n      <td>['though!  I expect the fellow we saw was the ...</td>\n      <td>{'Detective and mystery stories', 'British -- ...</td>\n    </tr>\n    <tr>\n      <th>161626</th>\n      <td>['all his fellows that I met, he was desperate...</td>\n      <td>{'World War, 1914-1918', 'Great Britain. Army ...</td>\n    </tr>\n    <tr>\n      <th>78287</th>\n      <td>['     _Which Contains Some Rather Painful Exp...</td>\n      <td>{'Aristocracy (Social class) -- Fiction', 'Dan...</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv('final.csv', usecols=['text', 'subjects'])\n",
    "data_frame = data_frame.sample(n=300, random_state=42)\n",
    "data_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:21:54.639565Z",
     "end_time": "2023-04-06T14:21:58.270164Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  \\\n110438  ['   Stands the castle of the maid;', 'The hou...   \n28916   ['\"Oh, Mr Gresham, such good friends as you an...   \n12917   ['', 'PROSPERO.', 'Ay, with a twink.', '', 'AR...   \n25817   ['Widow Rouleau.”', '', '“Ah! you know the poo...   \n43411   ['feel drawn to Moscow. So far I have not had ...   \n...                                                   ...   \n119879  ['glass is the cause of our present determined...   \n103694  ['', '\"Good-by, and good luck, Sam!\" said Henr...   \n131932  ['    Those parted lips that faintly smile--',...   \n146867  ['\"It\\'s a good cause,\" she said. \"I often thi...   \n121958  ['white-skinned woman and struck him down with...   \n\n                                                 subjects  \n110438                              {'Poetry', 'Ballads'}  \n28916   {'Physicians -- Fiction', 'Barsetshire (Englan...  \n12917   {'Fathers and daughters -- Drama', 'Political ...  \n25817                   {'Detective and mystery stories'}  \n43411   {'Chekhov, Anton Pavlovich, 1860-1904 -- Corre...  \n...                                                   ...  \n119879    {'German fiction -- Translations into English'}  \n103694  {'Conduct of life -- Fiction', 'New York (N.Y....  \n131932                                         {'Poetry'}  \n146867  {'Family -- Juvenile fiction', 'United States ...  \n121958  {'Conan (Fictitious character) -- Fiction', 'F...  \n\n[173107 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>subjects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>110438</th>\n      <td>['   Stands the castle of the maid;', 'The hou...</td>\n      <td>{'Poetry', 'Ballads'}</td>\n    </tr>\n    <tr>\n      <th>28916</th>\n      <td>['\"Oh, Mr Gresham, such good friends as you an...</td>\n      <td>{'Physicians -- Fiction', 'Barsetshire (Englan...</td>\n    </tr>\n    <tr>\n      <th>12917</th>\n      <td>['', 'PROSPERO.', 'Ay, with a twink.', '', 'AR...</td>\n      <td>{'Fathers and daughters -- Drama', 'Political ...</td>\n    </tr>\n    <tr>\n      <th>25817</th>\n      <td>['Widow Rouleau.”', '', '“Ah! you know the poo...</td>\n      <td>{'Detective and mystery stories'}</td>\n    </tr>\n    <tr>\n      <th>43411</th>\n      <td>['feel drawn to Moscow. So far I have not had ...</td>\n      <td>{'Chekhov, Anton Pavlovich, 1860-1904 -- Corre...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>119879</th>\n      <td>['glass is the cause of our present determined...</td>\n      <td>{'German fiction -- Translations into English'}</td>\n    </tr>\n    <tr>\n      <th>103694</th>\n      <td>['', '\"Good-by, and good luck, Sam!\" said Henr...</td>\n      <td>{'Conduct of life -- Fiction', 'New York (N.Y....</td>\n    </tr>\n    <tr>\n      <th>131932</th>\n      <td>['    Those parted lips that faintly smile--',...</td>\n      <td>{'Poetry'}</td>\n    </tr>\n    <tr>\n      <th>146867</th>\n      <td>['\"It\\'s a good cause,\" she said. \"I often thi...</td>\n      <td>{'Family -- Juvenile fiction', 'United States ...</td>\n    </tr>\n    <tr>\n      <th>121958</th>\n      <td>['white-skinned woman and struck him down with...</td>\n      <td>{'Conan (Fictitious character) -- Fiction', 'F...</td>\n    </tr>\n  </tbody>\n</table>\n<p>173107 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed=42\n",
    "data_frame = shuffle(data_frame,random_state=seed)\n",
    "data_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:11:54.045602Z",
     "end_time": "2023-04-06T12:11:54.100625Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply Label Encoding on the Books' subjects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y_actual= label_encoder.fit_transform(data_frame['subjects'])\n",
    "data_frame['subjects']=y_actual"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:11:54.063520Z",
     "end_time": "2023-04-06T12:11:54.101025Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW = CountVectorizer()\n",
    "BOW_transformation = BOW.fit_transform(data_frame['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:11:54.110817Z",
     "end_time": "2023-04-06T12:12:17.842932Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_ngram(n_gram,X_train=data_frame['text']):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n",
    "    x_train_vec = vectorizer.fit_transform(X_train)\n",
    "    return x_train_vec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:12:17.640501Z",
     "end_time": "2023-04-06T12:12:17.843125Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying tfidf with 1-gram, 2-gram and 3-gram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Applying tfidf with 1-gram, and 2-gram\n",
    "tfidf_1g_transformation= tfidf_ngram(1,X_train=data_frame['text'])\n",
    "tfidf_2g_transformation= tfidf_ngram(2,X_train=data_frame['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:12:17.705724Z",
     "end_time": "2023-04-06T12:14:07.553275Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Doc2Vec\n",
    "## Using BERT as pretrained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU\n",
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T12:14:07.454485Z",
     "end_time": "2023-04-06T12:14:07.553554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Libraries\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TensorDataset, DataLoader\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = data_frame['text'].values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = data_frame.subjects.values\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Tokenize the sentences and put them in the list tokenized_texts\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "hidden_states=[]\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "# Conver the ids into a tensor representation\n",
    "batch_size = 4\n",
    "input_tensor = torch.tensor(input_ids)\n",
    "masks_tensor = torch.tensor(attention_masks)\n",
    "train_data = TensorDataset(input_tensor, masks_tensor)\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "# Initialize the model\n",
    "if torch.cuda.is_available():\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True,).to('cuda')\n",
    "else:\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True,).to('cpu')\n",
    "model.eval()\n",
    "outputs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for input, masks in dataloader:\n",
    "    torch.cuda.empty_cache() # empty the gpu memory\n",
    "    # Transfer the batch to gp\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.to('cuda')\n",
    "        masks = masks.to('cuda')\n",
    "    # Run inference on the batch\n",
    "    output = model(input, attention_mask=masks)\n",
    "    # Transfer the output to CPU again and convert to numpy\n",
    "    output = output[0].cpu().detach().numpy()\n",
    "    # Store the output in a list\n",
    "    outputs.append(output)\n",
    "# Concatenate all the lists within the list into one list\n",
    "outputs = [x for y in outputs for x in y]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Embed the full sectence by taking the mean of the embedding vectors of the tokenized words**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_vectors=np.array(outputs)\n",
    "bert_vectors=bert_vectors.mean(axis=1)\n",
    "bert_vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "def get_vectors_pretrained(df, model):\n",
    "    embedding_vectors = []\n",
    "    for partition in df['Sample of the book']:\n",
    "        sentence = []\n",
    "        for word in partition.split(' '):\n",
    "            try:\n",
    "                sentence.append(model[word])\n",
    "            except:\n",
    "                pass\n",
    "        sentence = np.array(sentence)\n",
    "        sentence = sentence.mean(axis=0)\n",
    "        embedding_vectors.append(sentence)\n",
    "    embedding_vectors = np.array(embedding_vectors)\n",
    "    return embedding_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Glove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n",
    "glove_vectors=get_vectors_pretrained(data_frame,glove_model)\n",
    "glove_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fast text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "fast_text_model = api.load(\"fasttext-wiki-news-subwords-300\")  # load glove vectors\n",
    "fast_text_vectors=get_vectors_pretrained(data_frame,fast_text_model)\n",
    "fast_text_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # load glove vectors\n",
    "word2vec_vectors = get_vectors_pretrained(data_frame,word2vec_model)\n",
    "word2vec_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "paragraphs = data_frame[\"Sample of the book\"].to_list()\n",
    "docs = []\n",
    "\n",
    "for sen in paragraphs:\n",
    "    docs.append(list(sen.split()))\n",
    "print(len(docs))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print(len(corpus[2]))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "#print(len(dictionary))\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_topics = model.get_document_topics(corpus)\n",
    "num_docs = len(all_topics)\n",
    "\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "lda_to_cluster = all_topics_csr.T.toarray()\n",
    "lda_to_cluster.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Measure the coherence per topic of the LDA model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "## Evaluating coherence of gensim LDA model\n",
    "cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "coherence_score = cm.get_coherence()\n",
    "print(coherence_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word embedding dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_embedding={\n",
    "    'BOW':BOW_transformation.toarray(),\n",
    "    'TF_IDF 1_gram':tfidf_1g_transformation.toarray(),\n",
    "    'Doc2vec':np.array(doc2vec_vectors),\n",
    "    'Glove':glove_vectors,\n",
    "    'FastText':fast_text_vectors,\n",
    "    'Word2vec':word2vec_vectors,\n",
    "    'BERT':bert_vectors,\n",
    "    'LDA':lda_to_cluster,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save word Embedding as a pickle file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"EmbeddingText_edited.pkl\", \"wb\")\n",
    "pickle.dump(text_embedding, a_file)\n",
    "a_file.close()\n",
    "print('Saved')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Word embedding visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2,)\n",
    "embedding=text_embedding.copy()\n",
    "\n",
    "for key in embedding.keys():\n",
    "    embedding[key]=pca.fit_transform(embedding[key])\n",
    "    df=pd.DataFrame({'PCA1':embedding[key][:,0],'PCA2':embedding[key][:,1],'Target':y_actual})\n",
    "    fig = px.scatter(data_frame =df, x='PCA1', y='PCA2', color='Target')\n",
    "    fig.update_layout(title={'text':f'{key}','x':0.5},height=500,width=700)\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mapping function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def label_mapping(num, y_actual, y_target, df_labels):\n",
    "    if num == df_labels[df_labels[y_actual]==0][y_target].value_counts().idxmax():\n",
    "        return 0\n",
    "    if num == df_labels[df_labels[y_actual]==1][y_target].value_counts().idxmax():\n",
    "        return 1\n",
    "    if num == df_labels[df_labels[y_actual]==2][y_target].value_counts().idxmax():\n",
    "        return 2\n",
    "    if num == df_labels[df_labels[y_actual]==3][y_target].value_counts().idxmax():\n",
    "        return 3\n",
    "    if num ==df_labels[df_labels[y_actual]==4][y_target].value_counts().idxmax():\n",
    "        return 4\n",
    "    else :\n",
    "        return num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lab(df):\n",
    "    for i in range(5):\n",
    "        cluster_idx=df[df['y_pred']==i].index.to_list()\n",
    "        if len(cluster_idx)>0:\n",
    "            right_cluster=df.loc[cluster_idx,'y_actual'].mode()[0]\n",
    "            df['y_pred']=df['y_pred'].apply(lambda x: right_cluster+1000 if x==i else x)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Means"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "import warnings\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_embedding.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to apply k-means by passing the maximum number of clusters and the data\n",
    "# Retrieving two dictionaries; first one has each K-value with its wcss and the latter has each K-value with its silhouette score\n",
    "def run_KMeans(max_k, data):\n",
    "    max_k+=1\n",
    "    kmeans_elb_results = dict()\n",
    "    kmeans_sil_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmeans = KMeans(n_clusters = k\n",
    "                        , init = 'k-means++'\n",
    "                        , n_init = 10\n",
    "                        , max_iter=300\n",
    "                        , random_state = seed\n",
    "                        , algorithm = 'full')\n",
    "        kmeans.fit(data)\n",
    "        kmeans_elb_results.update( {k : kmeans.inertia_} )\n",
    "        kmeans_sil_results.update({k:silhouette_score(data, kmeans.labels_)})\n",
    "\n",
    "    return  kmeans_elb_results, kmeans_sil_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating dictionary of each k-values for each vectorizer\n",
    "seed = 42\n",
    "kmeans_elb_vec = dict()\n",
    "kmeans_sil_vec = dict()\n",
    "for k,v in text_embedding.items():\n",
    "    kmeans_elb_results, kmeans_sil_results = run_KMeans(10, v)\n",
    "    kmeans_elb_vec[k] = kmeans_elb_results\n",
    "    kmeans_sil_vec[k] = kmeans_sil_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the Elbow Method to get the best number of clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=3,horizontal_spacing=0.1,vertical_spacing=0.1);\n",
    "i=j=1\n",
    "for elb_vec_key, elb_vec_value in kmeans_elb_vec.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(elb_vec_value.keys()),\n",
    "            y=list(elb_vec_value.values()),\n",
    "            mode='lines',\n",
    "            name=elb_vec_key,\n",
    "        ),row=i, col=j)\n",
    "    j+=1\n",
    "    if j == 4:\n",
    "        j=1\n",
    "        i+=1\n",
    "fig.update_layout(height=1000, width=1000,title={'text':'K Means for each vectorizer using Elbow Method','x':0.5})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the Silhouette Method to get the best number of clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=3,horizontal_spacing=0.1,vertical_spacing=0.1);\n",
    "i=j=1\n",
    "for sil_vec_key, sil_vec_value in kmeans_sil_vec.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(sil_vec_value.keys()),\n",
    "            y=list(sil_vec_value.values()),\n",
    "            mode='lines',\n",
    "            name=sil_vec_key,\n",
    "        ),row=i, col=j)\n",
    "    j+=1\n",
    "    if j == 4:\n",
    "        j=1\n",
    "        i+=1\n",
    "fig.update_layout(height=1000, width=1000,title={'text':'K Means for each vectorizer using Silhouette Method','x':0.5})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose the best Word embedding technique based on the highest Cohen's Kappa score at k =5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vecs = list(text_embedding.keys())\n",
    "vec_kappa = dict()\n",
    "for vec in vecs:\n",
    "    kmeans_doc2vec = KMeans(n_clusters = 5\n",
    "                            , init = 'k-means++'\n",
    "                            , n_init = 10\n",
    "                            , max_iter=300\n",
    "                            , random_state = seed\n",
    "                            , algorithm = 'full')\n",
    "    kmeans_doc2vec.fit(text_embedding[vec])\n",
    "    y_pridect = kmeans_doc2vec.predict(text_embedding[vec])\n",
    "    y_df = pd.DataFrame({'y_actual':y_actual, 'y_pred':y_pridect})\n",
    "    y_df['y_pred'] = y_df['y_pred'].apply(lambda val: label_mapping(num=val, y_actual ='y_actual', y_target='y_pred', df_labels=y_df))\n",
    "    # y_df['y_pred']=y_df['y_pred'].apply(lambda x : x-1000 if x > 20 else x)\n",
    "    # y_df=lab(y_df)\n",
    "    kappa_score = cohen_kappa_score(y_df['y_actual'], y_df['y_pred'])\n",
    "    vec_kappa[vec] = kappa_score\n",
    "vec_kappa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "obj = go.Bar(x = list(vec_kappa.keys()), y = list(vec_kappa.values()))\n",
    "fig.add_trace(obj)\n",
    "fig.update_layout(title={'text':'Kappa Score for Different Vectorizers','x':0.5},height=600,width=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clusters Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### applying pca"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_PCA(data):\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(data)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "                               , columns = ['principal component 1', 'principal component 2'])\n",
    "    return principalDf\n",
    "pca_df = apply_PCA(text_embedding['Doc2vec'])\n",
    "pca_df_final = pd.concat([pca_df, pd.Series(y_actual,name='target')],axis=1)\n",
    "pca_df_final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Apply K-Means on the PCA transformed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', n_init = 10, max_iter=300, random_state = seed, algorithm = 'full')\n",
    "kmeans.fit(pca_df)\n",
    "kmeans_labels = kmeans.predict(pca_df)\n",
    "kmeans_df_final = pd.concat([pca_df, pd.Series(kmeans_labels,name='target')],axis=1)\n",
    "centroids = kmeans.cluster_centers_\n",
    "kmeans_df_final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Map the predicted clusters to the proper Labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_labels1 = pd.DataFrame({'Labels':pca_df_final['target'], 'Predicted Labels': list(kmeans_df_final['target']) })\n",
    "df_labels1['Predicted Labels'] = df_labels1['Predicted Labels'].apply(lambda val: label_mapping(num=val, y_actual ='Labels', y_target='Predicted Labels', df_labels=df_labels1))\n",
    "kmeans_df_final['target'] = df_labels1['Predicted Labels']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "fig1 = px.scatter(data_frame =pca_df_final, x='principal component 1', y='principal component 2', color='target')\n",
    "fig1.update_layout({'title':{'text': 'The Clusters with Actual Labels','x':0.5}},height=600,width=600)\n",
    "fig1.show()\n",
    "fig2 = px.scatter(data_frame =kmeans_df_final, x='principal component 1', y='principal component 2', color='target')\n",
    "fig2.update_layout(annotations=[\n",
    "    dict(\n",
    "        font=dict(color='black',size=30),\n",
    "        showarrow=False,\n",
    "        x=centroids[0][0],\n",
    "        y=centroids[0][1],\n",
    "        text=\"X\"),\n",
    "    dict(\n",
    "        font=dict(color='black',size=30),\n",
    "        showarrow=False,\n",
    "        x=centroids[1][0],\n",
    "        y=centroids[1][1],\n",
    "        text=\"X\",\n",
    "    ),\n",
    "    dict(font=dict(color='black',size=30),\n",
    "         showarrow=False,\n",
    "         x=centroids[2][0],\n",
    "         y=centroids[2][1],\n",
    "         text=\"X\",\n",
    "         ),\n",
    "    dict(\n",
    "        font=dict(color='black',size=30),\n",
    "        showarrow=False,\n",
    "        x=centroids[3][0],\n",
    "        y=centroids[3][1],\n",
    "        text=\"X\",\n",
    "    ),dict(\n",
    "        font=dict(color='black',size=30),\n",
    "        showarrow=False,\n",
    "        x=centroids[4][0],\n",
    "        y=centroids[4][1],\n",
    "        text=\"X\",\n",
    "    )],height=600,width=600)\n",
    "fig2.update_layout({'title':{'text': 'The Clusters with K-means','x':0.5},},height=600,width=600)\n",
    "fig2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Expectation Maximization (EM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "EM clustering is to estimate the means and standard deviations for each cluster to maximize the likelihood of the observed data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "y_df = pd.DataFrame({'y_actual':y_actual,'y_pred':y_actual})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Important Note:** for BOW and TF_IDF we should perform PCA to decrease number of features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=1000,)\n",
    "text_embedding_pca=text_embedding.copy()\n",
    "text_embedding_pca['BOW']=pca.fit_transform(text_embedding['BOW'])\n",
    "text_embedding_pca['TF_IDF 1_gram']=pca.fit_transform(text_embedding['TF_IDF 1_gram'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Use the Silhouette Method to get the best number of clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score,cohen_kappa_score\n",
    "n_clusters = np.arange(2,10)\n",
    "\n",
    "silhouette_for_each_transformation=text_embedding_pca.copy()\n",
    "for key in silhouette_for_each_transformation.keys():\n",
    "    silhouette_for_each_transformation[key]=[]\n",
    "\n",
    "bic_for_each_transformation=text_embedding_pca.copy()\n",
    "for key in bic_for_each_transformation.keys():\n",
    "    bic_for_each_transformation[key]=[]\n",
    "\n",
    "\n",
    "labels_for_each_transformation=text_embedding_pca.copy()\n",
    "for key in labels_for_each_transformation.keys():\n",
    "    labels_for_each_transformation[key]=[]\n",
    "\n",
    "kappa_for_each_transformation=text_embedding_pca.copy()\n",
    "\n",
    "y_df = pd.DataFrame({'y_actual':y_actual,'y_pred':y_actual})\n",
    "for key in text_embedding_pca.keys():\n",
    "    silhouette_scores=[]\n",
    "    bic=[]\n",
    "    aic=[]\n",
    "    kappa_at_5=[]\n",
    "    labels_list=[]\n",
    "    for n in n_clusters :\n",
    "        model=GaussianMixture(n, covariance_type='full', random_state=seed)\n",
    "        labels=model.fit_predict(text_embedding_pca[key])\n",
    "        silhouette_scores.append(silhouette_score(text_embedding_pca[key],labels=labels))\n",
    "        if n==5 :\n",
    "            y_df['y_pred']=labels\n",
    "            y_df['y_pred']=y_df['y_pred'].apply(lambda row: label_mapping(num=row,y_actual = 'y_actual',y_target='y_pred',df_labels=y_df ))\n",
    "            y_df['y_pred']=y_df['y_pred'].apply(lambda x: x-1000 if x>20 else x)\n",
    "            right_labels_mapping=y_df['y_pred']\n",
    "            kappa_at_5=cohen_kappa_score(y_actual,right_labels_mapping )\n",
    "        bic.append(model.bic(text_embedding_pca[key]))\n",
    "        labels_list.append(labels)\n",
    "    silhouette_for_each_transformation[key].append(silhouette_scores)\n",
    "    bic_for_each_transformation[key].append(bic)\n",
    "    kappa_for_each_transformation[key]=kappa_at_5\n",
    "    labels_for_each_transformation[key].append(labels_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx=[(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)]\n",
    "fig = make_subplots(rows=3, cols=3,horizontal_spacing=0.1,vertical_spacing=0.1);\n",
    "count=0\n",
    "for key in silhouette_for_each_transformation.keys() :\n",
    "    silhouette_scores=list(silhouette_for_each_transformation[key][0])\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(n_clusters),\n",
    "            y=list(silhouette_scores),\n",
    "            mode='lines',\n",
    "            name=key,\n",
    "        ),row=idx[count][0], col=idx[count][1])\n",
    "    count+=1\n",
    "fig.update_layout(height=1000, width=1000,title={'text':'Trying different K values to each transformation method and check the Silhouette score','x':0.5})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check kappa scores when k =5 with all transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kappa_for_each_transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "obj = go.Bar(x = list(kappa_for_each_transformation.keys()), y = list(kappa_for_each_transformation.values()))\n",
    "fig.add_trace(obj)\n",
    "fig.update_layout(title={'text':'Kappa Score for Different Vectorizers','x':0.5},height=600,width=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using PCA with the highest silhouette score to visualize clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2,)\n",
    "embedding=text_embedding.copy()\n",
    "key='Doc2vec'\n",
    "embedding[key]=pca.fit_transform(embedding[key])\n",
    "df=pd.DataFrame({'PCA1':embedding[key][:,0],'PCA2':embedding[key][:,1],'Target':labels_for_each_transformation[key][0][3]})\n",
    "fig = px.scatter(data_frame =df, x='PCA1', y='PCA2', color='Target')\n",
    "fig.update_layout(title={'text':f'{key}','x':0.5},height=500,width=500)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2,)\n",
    "embedding=text_embedding.copy()\n",
    "key='LDA'\n",
    "embedding[key]=pca.fit_transform(embedding[key])\n",
    "df=pd.DataFrame({'PCA1':embedding[key][:,0],'PCA2':embedding[key][:,1],'Target':labels_for_each_transformation[key][0][3]})\n",
    "fig = px.scatter(data_frame =df, x='PCA1', y='PCA2', color='Target')\n",
    "fig.update_layout(title={'text':f'{key}','x':0.5},height=500,width=500)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Hierarchical"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "def plot_cluster(model,X):\n",
    "    yhat = model.fit_predict(X)\n",
    "    clusters = unique(yhat)\n",
    "\n",
    "    tsne = TSNEVisualizer(random_state=seed)\n",
    "    tsne.fit(X, yhat)\n",
    "    tsne.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import SpectralClustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "# from yellowbrick.features import FeatureImportances\n",
    "# from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_size , y_size =3,3\n",
    "fig, axes = plt.subplots(3, 3,figsize=(20,20))\n",
    "\n",
    "model = AgglomerativeClustering()\n",
    "\n",
    "visualgrid = []\n",
    "for i in range(len(text_embedding.keys())):\n",
    "    ax = axes[i%x_size][i//y_size]\n",
    "    ax.set_title(text_embedding.keys())\n",
    "    visualgrid.append(KElbowVisualizer(model, k=(2,10), metric='distortion', timings=False, ax=axes[i%x_size][i//y_size], title = type(model).__name__ + ' with ' + list(text_embedding.keys())[i]))\n",
    "\n",
    "for vis,key in zip(visualgrid,text_embedding.keys()):\n",
    "    vis.fit(text_embedding[key])\n",
    "    vis.ax.set_title(key)\n",
    "    vis.finalize()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_size , y_size =3,3\n",
    "fig, axes = plt.subplots(3, 3,figsize=(20,20))\n",
    "model = AgglomerativeClustering()\n",
    "\n",
    "visualgrid = []\n",
    "for i in range(len(text_embedding.keys())):\n",
    "    ax = axes[i%x_size][i//y_size]\n",
    "    ax.set_title(text_embedding.keys())\n",
    "    visualgrid.append(KElbowVisualizer(model, k=(3,10), metric='silhouette', timings=False, ax=axes[i%x_size][i//y_size], title = type(model).__name__ + ' with ' + list(text_embedding.keys())[i]))\n",
    "\n",
    "for vis,key in zip(visualgrid,text_embedding.keys()):\n",
    "    vis.fit(text_embedding[key])\n",
    "    vis.ax.set_title(key)\n",
    "    vis.finalize()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "models = [AgglomerativeClustering(n_clusters=5),\n",
    "          # Birch(n_clusters=5),\n",
    "          # MiniBatchKMeans(n_clusters=5),\n",
    "          # KMeans(n_clusters=5),\n",
    "          # SpectralClustering(n_clusters=5)\n",
    "          ]\n",
    "\n",
    "models_scores = {}\n",
    "for model in models:\n",
    "    print(type(model).__name__)\n",
    "    kappa_scores = {}\n",
    "    for key in text_embedding.keys():\n",
    "        clustering = model.fit(text_embedding[key])\n",
    "        df_labels = pd.DataFrame({'y_actual':y_df['y_actual'], 'y_pred':clustering.labels_})\n",
    "        df_labels['y_pred'] = df_labels['y_pred'].apply(lambda val: label_mapping(num=val,y_actual = 'y_actual',y_target='y_pred',df_labels=df_labels ))\n",
    "        df_labels['y_pred']=df_labels['y_pred'].apply(lambda x: x-1000 if x>20 else x)\n",
    "        kappa_score = cohen_kappa_score(df_labels['y_actual'], df_labels['y_pred'])\n",
    "        kappa_scores[key] = kappa_score\n",
    "        print(key, ':', kappa_score)\n",
    "    models_scores[type(model).__name__] = kappa_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model_scores,model in zip(models_scores.keys(),models):\n",
    "    fig = go.Figure()\n",
    "    obj = go.Bar(x = list(models_scores[model_scores].keys()), y = list(models_scores[model_scores].values()))\n",
    "    fig.add_trace(obj)\n",
    "    fig.update_layout(title={'text':f'Kappa Score of the {type(model).__name__}','x':0.5},height=600,width=800)\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "dendrogram = sch.dendrogram(sch.linkage(text_embedding['Doc2vec'], method='ward'))\n",
    "hc = AgglomerativeClustering(n_clusters=5, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(text_embedding['Doc2vec'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
