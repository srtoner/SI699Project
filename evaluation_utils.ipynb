{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9603287-00f4-42c6-a3f8-e361410ea384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to compute optimal F-beta score using threshold tuning\n",
    "def compute_metrics(y_pred_prob, y_true):\n",
    "    \n",
    "    thresholds = np.arange(0, 1.01, 0.01)\n",
    "    results = pd.DataFrame(columns=['threshold', 'precision', 'recall', \n",
    "                                    'f-score'])\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = np.where(y_pred_prob >= threshold, 1, 0)\n",
    "          \n",
    "        # Compute precision\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        \n",
    "        # Compute recall\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        \n",
    "        # Compute F-beta score\n",
    "        f_score = fbeta_score(y_true, y_pred, beta=2)\n",
    "        \n",
    "        results = results.append({'threshold': threshold,\n",
    "                                  'precision': precision,\n",
    "                                  'recall': recall,\n",
    "                                  'f-score': f_score}, \n",
    "                                 ignore_index=True)\n",
    "        \n",
    "    return results.loc[results['f-score'] == results['f-score'].max(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803f84ab-e141-4f9d-aeda-d0796efbccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_area_under_precision_recall_curve(y_pred_prob, y_true):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_train, y_pred_prob)\n",
    "    auprc = auc(recall, precision)\n",
    "    return auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bdabbe-0439-4ee6-af04-ea2ad5fcdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_model_metrics(model, train_data, test_data, target_col, \n",
    "                            n_boot_samples):\n",
    "    \n",
    "    n_samples_train = train_data.shape[0]\n",
    "    n_samples_test = test_data.shape[0]\n",
    "    \n",
    "    train_auprc_boot_samples = np.zeros((n_boot_samples,))\n",
    "    train_precision_boot_samples = np.zeros((n_boot_samples,))\n",
    "    train_recall_boot_samples = np.zeros((n_boot_samples,))\n",
    "    train_fscore_boot_samples = np.zeros((n_boot_samples,))\n",
    "    \n",
    "    test_auprc_boot_samples = np.zeros((n_boot_samples,))\n",
    "    test_precision_boot_samples = np.zeros((n_boot_samples,))\n",
    "    test_recall_boot_samples = np.zeros((n_boot_samples,))\n",
    "    test_fscore_boot_samples = np.zeros((n_boot_samples,))\n",
    "    \n",
    "    for boot_iter in range(n_boot_samples):\n",
    "        # Generate bootstrap samples with replacement from training and test \n",
    "        # sets\n",
    "        boot_train_data = train_data[np.random.choice(n_samples_train, \n",
    "                                                      size=n_samples_train, \n",
    "                                                      replace=True), :]\n",
    "        boot_test_data = test_data[np.random.choice(n_samples_test, \n",
    "                                                    size=n_samples_test, \n",
    "                                                    replace=True), :]\n",
    "        \n",
    "        X_boot_train = boot_train.drop(columns=target_col)\n",
    "        X_boot_test = boot_test.drop(columns=target_col)\n",
    "        y_boot_train = boot_train[target_col]\n",
    "        y_boot_test = boot_test[target_col]\n",
    "        \n",
    "        model.fit(X_boot_train, y_boot_train)\n",
    "        \n",
    "        boot_train_pred_probs = model.predict_proba(X_boot_train)\n",
    "        boot_test_pred_probs = model.predict_proba(X_boot_test)\n",
    "        \n",
    "        boot_train_results = compute_metrics(boot_train_pred_probs, \n",
    "                                             y_boot_train)\n",
    "        boot_test_results = compute_metrics(boot_test_pred_probs, \n",
    "                                             y_boot_test)\n",
    "        \n",
    "        # Compute metrics on training set\n",
    "        train_auprc_boot_samples[boot_iter] = (\n",
    "            compute_area_under_precision_recall_curve(boot_train_pred_probs, \n",
    "                                                      y_boot_train)\n",
    "        )\n",
    "        train_precision_boot_samples[boot_iter] = (\n",
    "            boot_train_results['precision'].values[0]\n",
    "        )\n",
    "        train_recall_boot_samples[boot_iter] = (\n",
    "            boot_train_results['recall'].values[0]\n",
    "        )\n",
    "        train_fscore_boot_samples[boot_iter] = (\n",
    "            boot_train_results['f-score'].values[0]\n",
    "        )\n",
    "        \n",
    "        # Compute metrics on test set\n",
    "        test_auprc_boot_samples[boot_iter] = (\n",
    "            compute_area_under_precision_recall_curve(boot_test_pred_probs, \n",
    "                                                      y_boot_test)\n",
    "        )\n",
    "        test_precision_boot_samples[boot_iter] = (\n",
    "            boot_test_results['precision'].values[0]\n",
    "        )\n",
    "        test_recall_boot_samples[boot_iter] = (\n",
    "            boot_test_results['recall'].values[0]\n",
    "        )\n",
    "        test_fscore_boot_samples[boot_iter] = (\n",
    "            boot_test_results['f-score'].values[0]\n",
    "        )\n",
    "        \n",
    "    train_auprc_lower = np.quantile(train_auprc_boot_samples, 0.025)\n",
    "    train_auprc_upper = np.quantile(train_auprc_boot_samples, 0.975)\n",
    "    train_precision_lower = np.quantile(train_precision_boot_samples, 0.025)\n",
    "    train_precision_upper = np.quantile(train_precision_boot_samples, 0.975)\n",
    "    train_recall_lower = np.quantile(train_recall_boot_samples, 0.025)\n",
    "    train_recall_upper = np.quantile(train_recall_boot_samples, 0.975)\n",
    "    train_fscore_lower = np.quantile(train_fscore_boot_samples, 0.025)\n",
    "    train_fscore_upper = np.quantile(train_fscore_boot_samples, 0.975)\n",
    "    \n",
    "    test_auprc_lower = np.quantile(test_auprc_boot_samples, 0.025)\n",
    "    test_auprc_upper = np.quantile(test_auprc_boot_samples, 0.975)\n",
    "    test_precision_lower = np.quantile(test_precision_boot_samples, 0.025)\n",
    "    test_precision_upper = np.quantile(test_precision_boot_samples, 0.975)\n",
    "    test_recall_lower = np.quantile(test_recall_boot_samples, 0.025)\n",
    "    test_recall_upper = np.quantile(test_recall_boot_samples, 0.975)\n",
    "    test_fscore_lower = np.quantile(test_fscore_boot_samples, 0.025)\n",
    "    test_fscore_upper = np.quantile(test_fscore_boot_samples, 0.975)\n",
    "        \n",
    "    print(f'95% CI for train AUPRC: ({train_auprc_lower}, {train_auprc_upper})')\n",
    "    print(f'95% CI for train precision: ({train_precision_lower}, {train_precision_upper})')\n",
    "    print(f'95% CI for train recall: ({train_recall_lower}, {train_recall_upper})')\n",
    "    print(f'95% CI for train F-score: ({train_fscore_lower}, {train_fscore_upper}) \\n')\n",
    "    \n",
    "    print(f'95% CI for test AUPRC: ({test_auprc_lower}, {test_auprc_upper})')\n",
    "    print(f'95% CI for test precision: ({test_precision_lower}, {test_precision_upper})')\n",
    "    print(f'95% CI for test recall: ({test_recall_lower}, {test_recall_upper})')\n",
    "    print(f'95% CI for test F-score: ({test_fscore_lower}, {test_fscore_upper})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cbeeff-e341-4580-be4c-a2a55b6af9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(labels, pred_prob, plot_model_label):\n",
    "    \n",
    "    x, y = calibration_curve(labels, pred_prob, n_bins = 10)\n",
    "     \n",
    "    # Plot perfectly calibrated\n",
    "    plt.plot([0, 1], [0, 1], linestyle = '--', label = 'Ideally Calibrated')\n",
    "     \n",
    "    # Plot model's calibration curve\n",
    "    plt.plot(y, x, marker = '.', label = plot_model_label)\n",
    "     \n",
    "    leg = plt.legend(loc = 'lower right')\n",
    "    plt.xlabel('Average predicted probability in each bin')\n",
    "    plt.ylabel('Fraction of deceased patients')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40165c056c83275d9736ec999f27a3c7fc84a8d56239485103817be2cfc333e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
